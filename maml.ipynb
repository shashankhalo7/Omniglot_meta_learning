{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maml.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashankhalo7/Omniglot_meta_learning/blob/master/maml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG8AOER_ToFR",
        "colab_type": "code",
        "outputId": "13430e26-1469-43b1-ae07-20016bbf4327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!git clone https://github.com/brendenlake/omniglot.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'omniglot'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects:  33% (1/3)   \u001b[K\rremote: Counting objects:  66% (2/3)   \u001b[K\rremote: Counting objects: 100% (3/3)   \u001b[K\rremote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 81 (delta 0), reused 0 (delta 0), pack-reused 78\n",
            "Unpacking objects: 100% (81/81), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4mayU0OTxD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q omniglot/python/images_background.zip\n",
        "!unzip -q omniglot/python/images_evaluation.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV_SCBFZ4JpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!mkdir data/omniglot/\n",
        "!mkdir data/omniglot_resized/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr6Vzwvp3c6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r images_background/* data/omniglot_resized/\n",
        "!cp -r images_evaluation/* data/omniglot_resized/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2uqt4qT4u34",
        "colab_type": "code",
        "outputId": "0544c2e2-d54a-4bbc-fbb6-c0f95b4b39b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cp -r omniglot/* omniglot_resized/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: target 'omniglot_resized/' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0T8zdddUONM",
        "colab_type": "code",
        "outputId": "cd7209b8-b4e7-4a33-ea4f-0953074574da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "image_path = 'data/omniglot_resized/*/*/'\n",
        "\n",
        "all_images = glob.glob(image_path + '*')\n",
        "\n",
        "i = 0\n",
        "\n",
        "for image_file in all_images:\n",
        "    im = Image.open(image_file)\n",
        "    im = im.resize((28,28), resample=Image.LANCZOS)\n",
        "    im.save(image_file)\n",
        "    i += 1\n",
        "\n",
        "    if i % 200 == 0:\n",
        "        print(i)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "400\n",
            "600\n",
            "800\n",
            "1000\n",
            "1200\n",
            "1400\n",
            "1600\n",
            "1800\n",
            "2000\n",
            "2200\n",
            "2400\n",
            "2600\n",
            "2800\n",
            "3000\n",
            "3200\n",
            "3400\n",
            "3600\n",
            "3800\n",
            "4000\n",
            "4200\n",
            "4400\n",
            "4600\n",
            "4800\n",
            "5000\n",
            "5200\n",
            "5400\n",
            "5600\n",
            "5800\n",
            "6000\n",
            "6200\n",
            "6400\n",
            "6600\n",
            "6800\n",
            "7000\n",
            "7200\n",
            "7400\n",
            "7600\n",
            "7800\n",
            "8000\n",
            "8200\n",
            "8400\n",
            "8600\n",
            "8800\n",
            "9000\n",
            "9200\n",
            "9400\n",
            "9600\n",
            "9800\n",
            "10000\n",
            "10200\n",
            "10400\n",
            "10600\n",
            "10800\n",
            "11000\n",
            "11200\n",
            "11400\n",
            "11600\n",
            "11800\n",
            "12000\n",
            "12200\n",
            "12400\n",
            "12600\n",
            "12800\n",
            "13000\n",
            "13200\n",
            "13400\n",
            "13600\n",
            "13800\n",
            "14000\n",
            "14200\n",
            "14400\n",
            "14600\n",
            "14800\n",
            "15000\n",
            "15200\n",
            "15400\n",
            "15600\n",
            "15800\n",
            "16000\n",
            "16200\n",
            "16400\n",
            "16600\n",
            "16800\n",
            "17000\n",
            "17200\n",
            "17400\n",
            "17600\n",
            "17800\n",
            "18000\n",
            "18200\n",
            "18400\n",
            "18600\n",
            "18800\n",
            "19000\n",
            "19200\n",
            "19400\n",
            "19600\n",
            "19800\n",
            "20000\n",
            "20200\n",
            "20400\n",
            "20600\n",
            "20800\n",
            "21000\n",
            "21200\n",
            "21400\n",
            "21600\n",
            "21800\n",
            "22000\n",
            "22200\n",
            "22400\n",
            "22600\n",
            "22800\n",
            "23000\n",
            "23200\n",
            "23400\n",
            "23600\n",
            "23800\n",
            "24000\n",
            "24200\n",
            "24400\n",
            "24600\n",
            "24800\n",
            "25000\n",
            "25200\n",
            "25400\n",
            "25600\n",
            "25800\n",
            "26000\n",
            "26200\n",
            "26400\n",
            "26600\n",
            "26800\n",
            "27000\n",
            "27200\n",
            "27400\n",
            "27600\n",
            "27800\n",
            "28000\n",
            "28200\n",
            "28400\n",
            "28600\n",
            "28800\n",
            "29000\n",
            "29200\n",
            "29400\n",
            "29600\n",
            "29800\n",
            "30000\n",
            "30200\n",
            "30400\n",
            "30600\n",
            "30800\n",
            "31000\n",
            "31200\n",
            "31400\n",
            "31600\n",
            "31800\n",
            "32000\n",
            "32200\n",
            "32400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EkWjIaxUgIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.contrib.layers.python import layers as tf_layers\n",
        "from tensorflow.python.platform import flags\n",
        "\n",
        "from __future__ import print_function\n",
        "import sys\n",
        "import csv\n",
        "import pickle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kylyixcFNM4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS = flags.FLAGS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfMmz_qPNQob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Image helper\n",
        "def get_images(paths, labels, nb_samples=None, shuffle=True):\n",
        "    if nb_samples is not None:\n",
        "        sampler = lambda x: random.sample(x, nb_samples)\n",
        "    else:\n",
        "        sampler = lambda x: x\n",
        "    images = [(i, os.path.join(path, image)) \\\n",
        "        for i, path in zip(labels, paths) \\\n",
        "        for image in sampler(os.listdir(path))]\n",
        "    if shuffle:\n",
        "        random.shuffle(images)\n",
        "    return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKwW0_dtNRYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Network helpers\n",
        "def conv_block(inp, cweight, bweight, reuse, scope, activation=tf.nn.relu, max_pool_pad='VALID', residual=False):\n",
        "    \"\"\" Perform, conv, batch norm, nonlinearity, and max pool \"\"\"\n",
        "    stride, no_stride = [1,2,2,1], [1,1,1,1]\n",
        "\n",
        "    if FLAGS.max_pool:\n",
        "        conv_output = tf.nn.conv2d(inp, cweight, no_stride, 'SAME') + bweight\n",
        "    else:\n",
        "        conv_output = tf.nn.conv2d(inp, cweight, stride, 'SAME') + bweight\n",
        "    normed = normalize(conv_output, activation, reuse, scope)\n",
        "    if FLAGS.max_pool:\n",
        "        normed = tf.nn.max_pool(normed, stride, stride, max_pool_pad)\n",
        "    return normed\n",
        "\n",
        "def normalize(inp, activation, reuse, scope):\n",
        "  if FLAGS.norm == 'batch_norm':\n",
        "    return tf_layers.batch_norm(inp, activation_fn=activation, reuse=reuse, scope=scope)\n",
        "  elif FLAGS.norm == 'layer_norm':\n",
        "    return tf_layers.layer_norm(inp, activation_fn=activation, reuse=reuse, scope=scope)\n",
        "  elif FLAGS.norm == 'None':\n",
        "    if activation is not None:\n",
        "      return activation(inp)\n",
        "    else:\n",
        "      return inp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19b9qS3yNxvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Loss functions\n",
        "def mse(pred, label):\n",
        "    pred = tf.reshape(pred, [-1])\n",
        "    label = tf.reshape(label, [-1])\n",
        "    return tf.reduce_mean(tf.square(pred-label))\n",
        "\n",
        "def xent(pred, label):\n",
        "    # Note - with tf version <=0.12, this loss has incorrect 2nd derivatives\n",
        "    return tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=label) / FLAGS.update_batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncGEPP13N3tU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Code for loading data. \"\"\"\n",
        "\n",
        "class DataGenerator(object):\n",
        "    \"\"\"\n",
        "    Data Generator capable of generating batches of Omniglot data.\n",
        "    A \"class\" is considered a class of omniglot digits\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples_per_class, batch_size, config={}):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_samples_per_class: num samples to generate per class in one batch\n",
        "            batch_size: size of meta batch size (e.g. number of functions)\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.num_samples_per_class = num_samples_per_class\n",
        "        self.num_classes = 1  # by default 1 (only relevant for classification problems)\n",
        "        \n",
        "        if 'omniglot' in FLAGS.datasource:\n",
        "            self.num_classes = config.get('num_classes', FLAGS.num_classes)\n",
        "            self.img_size = config.get('img_size', (28, 28))\n",
        "            self.dim_input = np.prod(self.img_size)\n",
        "            self.dim_output = self.num_classes\n",
        "            # data that is pre-resized using PIL with lanczos filter\n",
        "            data_folder = config.get('data_folder', './data/omniglot_resized')\n",
        "\n",
        "            character_folders = [os.path.join(data_folder, family, character) \\\n",
        "                for family in os.listdir(data_folder) \\\n",
        "                if os.path.isdir(os.path.join(data_folder, family)) \\\n",
        "                for character in os.listdir(os.path.join(data_folder, family))]\n",
        "            random.seed(1)\n",
        "            random.shuffle(character_folders)\n",
        "            num_val = 100\n",
        "            num_train = config.get('num_train', 1200) - num_val\n",
        "            self.metatrain_character_folders = character_folders[:num_train]\n",
        "            if FLAGS.test_set:\n",
        "                self.metaval_character_folders = character_folders[num_train+num_val:]\n",
        "            else:\n",
        "                self.metaval_character_folders = character_folders[num_train:num_train+num_val]\n",
        "            self.rotations = config.get('rotations', [0, 90, 180, 270])\n",
        "        else:\n",
        "          raise ValueError('Unrecognized data source')\n",
        "\n",
        "\n",
        "    def make_data_tensor(self, train=True):\n",
        "        if train:\n",
        "            folders = self.metatrain_character_folders\n",
        "            # number of tasks, not number of meta-iterations. (divide by metabatch size to measure)\n",
        "            num_total_batches = 200000\n",
        "        else:\n",
        "            folders = self.metaval_character_folders\n",
        "            num_total_batches = 600\n",
        "\n",
        "        # make list of files\n",
        "        print('Generating filenames')\n",
        "        all_filenames = []\n",
        "        for _ in range(num_total_batches):\n",
        "            sampled_character_folders = random.sample(folders, self.num_classes)\n",
        "            random.shuffle(sampled_character_folders)\n",
        "            labels_and_images = get_images(sampled_character_folders, range(self.num_classes), nb_samples=self.num_samples_per_class, shuffle=False)\n",
        "            # make sure the above isn't randomized order\n",
        "            labels = [li[0] for li in labels_and_images]\n",
        "            filenames = [li[1] for li in labels_and_images]\n",
        "            all_filenames.extend(filenames)\n",
        "\n",
        "        # make queue for tensorflow to read from\n",
        "        filename_queue = tf.train.string_input_producer(tf.convert_to_tensor(all_filenames), shuffle=False)\n",
        "        print('Generating image processing ops')\n",
        "        image_reader = tf.WholeFileReader()\n",
        "        _, image_file = image_reader.read(filename_queue)\n",
        "        if FLAGS.datasource == 'omniglot':\n",
        "            image = tf.image.decode_png(image_file)\n",
        "            image.set_shape((self.img_size[0],self.img_size[1],1))\n",
        "            image = tf.reshape(image, [self.dim_input])\n",
        "            image = tf.cast(image, tf.float32) / 255.0\n",
        "            image = 1.0 - image  # invert\n",
        "        num_preprocess_threads = 1 # TODO - enable this to be set to >1\n",
        "        min_queue_examples = 256\n",
        "        examples_per_batch = self.num_classes * self.num_samples_per_class\n",
        "        batch_image_size = self.batch_size  * examples_per_batch\n",
        "        print('Batching images')\n",
        "        images = tf.train.batch(\n",
        "                [image],\n",
        "                batch_size = batch_image_size,\n",
        "                num_threads=num_preprocess_threads,\n",
        "                capacity=min_queue_examples + 3 * batch_image_size,\n",
        "                )\n",
        "        all_image_batches, all_label_batches = [], []\n",
        "        print('Manipulating image data to be right shape')\n",
        "        for i in range(self.batch_size):\n",
        "            image_batch = images[i*examples_per_batch:(i+1)*examples_per_batch]\n",
        "\n",
        "            if FLAGS.datasource == 'omniglot':\n",
        "                # omniglot augments the dataset by rotating digits to create new classes\n",
        "                # get rotation per class (e.g. 0,1,2,0,0 if there are 5 classes)\n",
        "                rotations = tf.multinomial(tf.log([[1., 1.,1.,1.]]), self.num_classes)\n",
        "            label_batch = tf.convert_to_tensor(labels)\n",
        "            new_list, new_label_list = [], []\n",
        "            for k in range(self.num_samples_per_class):\n",
        "                class_idxs = tf.range(0, self.num_classes)\n",
        "                class_idxs = tf.random_shuffle(class_idxs)\n",
        "\n",
        "                true_idxs = class_idxs*self.num_samples_per_class + k\n",
        "                new_list.append(tf.gather(image_batch,true_idxs))\n",
        "                if FLAGS.datasource == 'omniglot': # and FLAGS.train:\n",
        "                    new_list[-1] = tf.stack([tf.reshape(tf.image.rot90(\n",
        "                        tf.reshape(new_list[-1][ind], [self.img_size[0],self.img_size[1],1]),\n",
        "                        k=tf.cast(rotations[0,class_idxs[ind]], tf.int32)), (self.dim_input,))\n",
        "                        for ind in range(self.num_classes)])\n",
        "                new_label_list.append(tf.gather(label_batch, true_idxs))\n",
        "            new_list = tf.concat(new_list, 0)  # has shape [self.num_classes*self.num_samples_per_class, self.dim_input]\n",
        "            new_label_list = tf.concat(new_label_list, 0)\n",
        "            all_image_batches.append(new_list)\n",
        "            all_label_batches.append(new_label_list)\n",
        "        all_image_batches = tf.stack(all_image_batches)\n",
        "        all_label_batches = tf.stack(all_label_batches)\n",
        "        all_label_batches = tf.one_hot(all_label_batches, self.num_classes)\n",
        "        return all_image_batches, all_label_batches\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkTtlZMPOrqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MAML:\n",
        "    def __init__(self, dim_input=1, dim_output=1, test_num_updates=5):\n",
        "        \"\"\" must call construct_model() after initializing MAML! \"\"\"\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_output = dim_output\n",
        "        self.update_lr = FLAGS.update_lr\n",
        "        self.meta_lr = tf.placeholder_with_default(FLAGS.meta_lr, ())\n",
        "        self.classification = False\n",
        "        self.test_num_updates = test_num_updates\n",
        "        self.loss_func = xent\n",
        "        self.classification = True\n",
        "        self.channels = 1\n",
        "        if FLAGS.conv:\n",
        "            self.dim_hidden = FLAGS.num_filters\n",
        "            self.forward = self.forward_conv\n",
        "            self.construct_weights = self.construct_conv_weights\n",
        "        else:\n",
        "            self.dim_hidden = [256, 128, 64, 64]\n",
        "            self.forward=self.forward_fc\n",
        "            self.construct_weights = self.construct_fc_weights\n",
        "            self.channels = 1\n",
        "        self.img_size = int(np.sqrt(self.dim_input/self.channels))\n",
        "\n",
        "    def construct_model(self, input_tensors=None, prefix='metatrain_'):\n",
        "        # a: training data for inner gradient, b: test data for meta gradient\n",
        "        if input_tensors is None:\n",
        "            self.inputa = tf.placeholder(tf.float32) # traning data for base model\n",
        "            self.inputb = tf.placeholder(tf.float32) # testing for meta learner\n",
        "            self.labela = tf.placeholder(tf.float32) # labels for training data for the base model\n",
        "            self.labelb = tf.placeholder(tf.float32) #  labels for testing data for the meta learner\n",
        "        else:\n",
        "            self.inputa = input_tensors['inputa']\n",
        "            self.inputb = input_tensors['inputb']\n",
        "            self.labela = input_tensors['labela']\n",
        "            self.labelb = input_tensors['labelb']\n",
        "\n",
        "        with tf.variable_scope('model', reuse=None) as training_scope:\n",
        "            if 'weights' in dir(self):\n",
        "                training_scope.reuse_variables()\n",
        "                weights = self.weights\n",
        "            else:\n",
        "                # Define the weights\n",
        "                self.weights = weights = self.construct_weights()\n",
        "\n",
        "            # outputbs[i] and lossesb[i] is the output and loss after i+1 gradient updates\n",
        "            lossesa, outputas, lossesb, outputbs = [], [], [], []\n",
        "            accuraciesa, accuraciesb = [], []\n",
        "            num_updates = max(self.test_num_updates, FLAGS.num_updates)\n",
        "            outputbs = [[]]*num_updates\n",
        "            lossesb = [[]]*num_updates\n",
        "            accuraciesb = [[]]*num_updates\n",
        "\n",
        "            def task_metalearn(inp, reuse=True):\n",
        "                \"\"\" Perform gradient descent for one task in the meta-batch. \"\"\"\n",
        "                inputa, inputb, labela, labelb = inp\n",
        "                task_outputbs, task_lossesb = [], []\n",
        "\n",
        "                if self.classification:\n",
        "                    task_accuraciesb = []\n",
        "\n",
        "                task_outputa = self.forward(inputa, weights, reuse=reuse)  # only reuse on the first iter\n",
        "                task_lossa = self.loss_func(task_outputa, labela)\n",
        "                grads = tf.gradients(task_lossa, list(weights.values()))\n",
        "                if FLAGS.stop_grad:\n",
        "                    grads = [tf.stop_gradient(grad) for grad in grads]\n",
        "                gradients = dict(zip(weights.keys(), grads))\n",
        "                fast_weights = dict(zip(weights.keys(), [weights[key] - self.update_lr*gradients[key] for key in weights.keys()]))\n",
        "                output = self.forward(inputb, fast_weights, reuse=True)\n",
        "                task_outputbs.append(output)\n",
        "                task_lossesb.append(self.loss_func(output, labelb))\n",
        "\n",
        "                for j in range(num_updates - 1):\n",
        "                    loss = self.loss_func(self.forward(inputa, fast_weights, reuse=True), labela)\n",
        "                    grads = tf.gradients(loss, list(fast_weights.values()))\n",
        "                    if FLAGS.stop_grad:\n",
        "                        grads = [tf.stop_gradient(grad) for grad in grads]\n",
        "                    gradients = dict(zip(fast_weights.keys(), grads))\n",
        "                    fast_weights = dict(zip(fast_weights.keys(), [fast_weights[key] - self.update_lr*gradients[key] for key in fast_weights.keys()]))\n",
        "                    output = self.forward(inputb, fast_weights, reuse=True)\n",
        "                    task_outputbs.append(output)\n",
        "                    task_lossesb.append(self.loss_func(output, labelb))\n",
        "\n",
        "                task_output = [task_outputa, task_outputbs, task_lossa, task_lossesb]\n",
        "\n",
        "                if self.classification:\n",
        "                    task_accuracya = tf.contrib.metrics.accuracy(tf.argmax(tf.nn.softmax(task_outputa), 1), tf.argmax(labela, 1))\n",
        "                    for j in range(num_updates):\n",
        "                        task_accuraciesb.append(tf.contrib.metrics.accuracy(tf.argmax(tf.nn.softmax(task_outputbs[j]), 1), tf.argmax(labelb, 1)))\n",
        "                    task_output.extend([task_accuracya, task_accuraciesb])\n",
        "\n",
        "                return task_output\n",
        "\n",
        "            if FLAGS.norm is not 'None':\n",
        "                # to initialize the batch norm vars, might want to combine this, and not run idx 0 twice.\n",
        "                unused = task_metalearn((self.inputa[0], self.inputb[0], self.labela[0], self.labelb[0]), False)\n",
        "\n",
        "            out_dtype = [tf.float32, [tf.float32]*num_updates, tf.float32, [tf.float32]*num_updates]\n",
        "            if self.classification:\n",
        "                out_dtype.extend([tf.float32, [tf.float32]*num_updates])\n",
        "            result = tf.map_fn(task_metalearn, elems=(self.inputa, self.inputb, self.labela, self.labelb), dtype=out_dtype, parallel_iterations=FLAGS.meta_batch_size)\n",
        "            if self.classification:\n",
        "                outputas, outputbs, lossesa, lossesb, accuraciesa, accuraciesb = result\n",
        "            else:\n",
        "                outputas, outputbs, lossesa, lossesb  = result\n",
        "\n",
        "        ## Performance & Optimization\n",
        "        if 'train' in prefix:\n",
        "            self.total_loss1 = total_loss1 = tf.reduce_sum(lossesa) / tf.to_float(FLAGS.meta_batch_size)\n",
        "            self.total_losses2 = total_losses2 = [tf.reduce_sum(lossesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
        "            # after the map_fn\n",
        "            self.outputas, self.outputbs = outputas, outputbs\n",
        "            if self.classification:\n",
        "                self.total_accuracy1 = total_accuracy1 = tf.reduce_sum(accuraciesa) / tf.to_float(FLAGS.meta_batch_size)\n",
        "                self.total_accuracies2 = total_accuracies2 = [tf.reduce_sum(accuraciesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
        "            self.pretrain_op = tf.train.AdamOptimizer(self.meta_lr).minimize(total_loss1)\n",
        "\n",
        "            if FLAGS.metatrain_iterations > 0:\n",
        "                optimizer = tf.train.AdamOptimizer(self.meta_lr)\n",
        "                self.gvs = gvs = optimizer.compute_gradients(self.total_losses2[FLAGS.num_updates-1])\n",
        "                self.metatrain_op = optimizer.apply_gradients(gvs)\n",
        "        else:\n",
        "            self.metaval_total_loss1 = total_loss1 = tf.reduce_sum(lossesa) / tf.to_float(FLAGS.meta_batch_size)\n",
        "            self.metaval_total_losses2 = total_losses2 = [tf.reduce_sum(lossesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
        "            if self.classification:\n",
        "                self.metaval_total_accuracy1 = total_accuracy1 = tf.reduce_sum(accuraciesa) / tf.to_float(FLAGS.meta_batch_size)\n",
        "                self.metaval_total_accuracies2 = total_accuracies2 =[tf.reduce_sum(accuraciesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
        "\n",
        "        ## Summaries\n",
        "        tf.summary.scalar(prefix+'Pre-update loss', total_loss1)\n",
        "        if self.classification:\n",
        "            tf.summary.scalar(prefix+'Pre-update accuracy', total_accuracy1)\n",
        "\n",
        "        for j in range(num_updates):\n",
        "            tf.summary.scalar(prefix+'Post-update loss, step ' + str(j+1), total_losses2[j])\n",
        "            if self.classification:\n",
        "                tf.summary.scalar(prefix+'Post-update accuracy, step ' + str(j+1), total_accuracies2[j])\n",
        "\n",
        "    ### Network construction functions (fc networks and conv networks)\n",
        "    def construct_fc_weights(self):\n",
        "        weights = {}\n",
        "        weights['w1'] = tf.Variable(tf.truncated_normal([self.dim_input, self.dim_hidden[0]], stddev=0.01))\n",
        "        weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden[0]]))\n",
        "        for i in range(1,len(self.dim_hidden)):\n",
        "            weights['w'+str(i+1)] = tf.Variable(tf.truncated_normal([self.dim_hidden[i-1], self.dim_hidden[i]], stddev=0.01))\n",
        "            weights['b'+str(i+1)] = tf.Variable(tf.zeros([self.dim_hidden[i]]))\n",
        "        weights['w'+str(len(self.dim_hidden)+1)] = tf.Variable(tf.truncated_normal([self.dim_hidden[-1], self.dim_output], stddev=0.01))\n",
        "        weights['b'+str(len(self.dim_hidden)+1)] = tf.Variable(tf.zeros([self.dim_output]))\n",
        "        return weights\n",
        "\n",
        "    def forward_fc(self, inp, weights, reuse=False):\n",
        "        hidden = normalize(tf.matmul(inp, weights['w1']) + weights['b1'], activation=tf.nn.relu, reuse=reuse, scope='0')\n",
        "        for i in range(1,len(self.dim_hidden)):\n",
        "            hidden = normalize(tf.matmul(hidden, weights['w'+str(i+1)]) + weights['b'+str(i+1)], activation=tf.nn.relu, reuse=reuse, scope=str(i+1))\n",
        "        return tf.matmul(hidden, weights['w'+str(len(self.dim_hidden)+1)]) + weights['b'+str(len(self.dim_hidden)+1)]\n",
        "\n",
        "    def construct_conv_weights(self):\n",
        "        weights = {}\n",
        "\n",
        "        dtype = tf.float32\n",
        "        conv_initializer =  tf.contrib.layers.xavier_initializer_conv2d(dtype=dtype)\n",
        "        fc_initializer =  tf.contrib.layers.xavier_initializer(dtype=dtype)\n",
        "        k = 3\n",
        "\n",
        "        weights['conv1'] = tf.get_variable('conv1', [k, k, self.channels, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
        "        weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
        "        weights['conv2'] = tf.get_variable('conv2', [k, k, self.dim_hidden, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
        "        weights['b2'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
        "        weights['conv3'] = tf.get_variable('conv3', [k, k, self.dim_hidden, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
        "        weights['b3'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
        "        weights['conv4'] = tf.get_variable('conv4', [k, k, self.dim_hidden, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
        "        weights['b4'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
        "        if FLAGS.datasource == 'omniglot':\n",
        "            weights['w5'] = tf.Variable(tf.random_normal([self.dim_hidden, self.dim_output]), name='w5')\n",
        "            weights['b5'] = tf.Variable(tf.zeros([self.dim_output]), name='b5')\n",
        "        return weights\n",
        "\n",
        "    def forward_conv(self, inp, weights, reuse=False, scope=''):\n",
        "        # reuse is for the normalization parameters.\n",
        "        channels = self.channels\n",
        "        inp = tf.reshape(inp, [-1, self.img_size, self.img_size, channels])\n",
        "\n",
        "        hidden1 = conv_block(inp, weights['conv1'], weights['b1'], reuse, scope+'0')\n",
        "        hidden2 = conv_block(hidden1, weights['conv2'], weights['b2'], reuse, scope+'1')\n",
        "        hidden3 = conv_block(hidden2, weights['conv3'], weights['b3'], reuse, scope+'2')\n",
        "        hidden4 = conv_block(hidden3, weights['conv4'], weights['b4'], reuse, scope+'3')\n",
        "        if FLAGS.datasource == 'omniglot':\n",
        "            hidden4 = tf.reduce_mean(hidden4, [1, 2])\n",
        "\n",
        "        return tf.matmul(hidden4, weights['w5']) + weights['b5']\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ6tePtuPrDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Dataset/method options\n",
        "flags.DEFINE_string('datasource', 'omniglot', 'The dataset name')\n",
        "flags.DEFINE_integer('num_classes', 5, 'number of classes used in classification (e.g. 5-way classification).')\n",
        "# oracle means task id is input (only suitable for sinusoid)\n",
        "flags.DEFINE_string('baseline', None, 'oracle, or None')\n",
        "\n",
        "## Training options\n",
        "flags.DEFINE_integer('pretrain_iterations', 0, 'number of pre-training iterations.')\n",
        "flags.DEFINE_integer('metatrain_iterations', 15000, 'number of metatraining iterations.') # 15k for omniglot\n",
        "flags.DEFINE_integer('meta_batch_size', 25, 'number of tasks sampled per meta-update')\n",
        "flags.DEFINE_float('meta_lr', 0.001, 'the base learning rate of the generator')\n",
        "flags.DEFINE_integer('update_batch_size', 5, 'number of examples used for inner gradient update (K for K-shot learning).')\n",
        "flags.DEFINE_float('update_lr', 1e-3, 'step size alpha for inner gradient update.') # 0.1 for omniglot\n",
        "flags.DEFINE_integer('num_updates', 1, 'number of inner gradient updates during training.')\n",
        "\n",
        "## Model options\n",
        "flags.DEFINE_string('norm', 'batch_norm', 'batch_norm, layer_norm, or None')\n",
        "flags.DEFINE_integer('num_filters', 64, 'number of filters for conv nets -- 32 for miniimagenet, 64 for omiglot.')\n",
        "flags.DEFINE_bool('conv', True, 'whether or not to use a convolutional network, only applicable in some cases')\n",
        "flags.DEFINE_bool('max_pool', False, 'Whether or not to use max pooling rather than strided convolutions')\n",
        "flags.DEFINE_bool('stop_grad', False, 'if True, do not use second derivatives in meta-optimization (for speed)')\n",
        "\n",
        "## Logging, saving, and testing options\n",
        "flags.DEFINE_bool('log', True, 'if false, do not log summaries, for debugging code.')\n",
        "flags.DEFINE_string('logdir', '/tmp/data', 'directory for summaries and checkpoints.')\n",
        "flags.DEFINE_bool('resume', True, 'resume training if there is a model available')\n",
        "flags.DEFINE_bool('train', True, 'True to train, False to test.')\n",
        "flags.DEFINE_integer('test_iter', -1, 'iteration to load model (-1 for latest model)')\n",
        "flags.DEFINE_bool('test_set', False, 'Set to true to test on the the test set, False for the validation set.')\n",
        "flags.DEFINE_integer('train_update_batch_size', -1, 'number of examples used for gradient update during training (use if you want to test with a different number).')\n",
        "flags.DEFINE_float('train_update_lr', -1, 'value of inner gradient step step during training. (use if you want to test with a different value)') # 0.1 for omniglot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-47yadFP-SI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, saver, sess, exp_string, data_generator, resume_itr=0):\n",
        "    SUMMARY_INTERVAL = 100\n",
        "    SAVE_INTERVAL = 1000\n",
        "    PRINT_INTERVAL = 100\n",
        "    TEST_PRINT_INTERVAL = PRINT_INTERVAL*5\n",
        "    if FLAGS.log:\n",
        "        train_writer = tf.summary.FileWriter(FLAGS.logdir + '/' + exp_string, sess.graph)\n",
        "    print('Done initializing, starting training.')\n",
        "    prelosses, postlosses = [], []\n",
        "\n",
        "    num_classes = data_generator.num_classes # for classification, 1 otherwise\n",
        "    multitask_weights, reg_weights = [], []\n",
        "\n",
        "    for itr in range(resume_itr, FLAGS.pretrain_iterations + FLAGS.metatrain_iterations):\n",
        "        feed_dict = {}\n",
        "        if 'generate' in dir(data_generator):\n",
        "            batch_x, batch_y, amp, phase = data_generator.generate()\n",
        "\n",
        "            if FLAGS.baseline == 'oracle':\n",
        "                batch_x = np.concatenate([batch_x, np.zeros([batch_x.shape[0], batch_x.shape[1], 2])], 2)\n",
        "                for i in range(FLAGS.meta_batch_size):\n",
        "                    batch_x[i, :, 1] = amp[i]\n",
        "                    batch_x[i, :, 2] = phase[i]\n",
        "\n",
        "            inputa = batch_x[:, :num_classes*FLAGS.update_batch_size, :]\n",
        "            labela = batch_y[:, :num_classes*FLAGS.update_batch_size, :]\n",
        "            inputb = batch_x[:, num_classes*FLAGS.update_batch_size:, :] # b used for testing\n",
        "            labelb = batch_y[:, num_classes*FLAGS.update_batch_size:, :]\n",
        "            feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb}\n",
        "\n",
        "        if itr < FLAGS.pretrain_iterations:\n",
        "            input_tensors = [model.pretrain_op]\n",
        "        else:\n",
        "            input_tensors = [model.metatrain_op]\n",
        "\n",
        "        if (itr % SUMMARY_INTERVAL == 0 or itr % PRINT_INTERVAL == 0):\n",
        "            input_tensors.extend([model.summ_op, model.total_loss1, model.total_losses2[FLAGS.num_updates-1]])\n",
        "            if model.classification:\n",
        "                input_tensors.extend([model.total_accuracy1, model.total_accuracies2[FLAGS.num_updates-1]])\n",
        "\n",
        "        result = sess.run(input_tensors, feed_dict)\n",
        "\n",
        "        if itr % SUMMARY_INTERVAL == 0:\n",
        "            prelosses.append(result[-2])\n",
        "            if FLAGS.log:\n",
        "                train_writer.add_summary(result[1], itr)\n",
        "            postlosses.append(result[-1])\n",
        "\n",
        "        if (itr!=0) and itr % PRINT_INTERVAL == 0:\n",
        "            if itr < FLAGS.pretrain_iterations:\n",
        "                print_str = 'Pretrain Iteration ' + str(itr)\n",
        "            else:\n",
        "                print_str = 'Iteration ' + str(itr - FLAGS.pretrain_iterations)\n",
        "            print_str += ': ' + str(np.mean(prelosses)) + ', ' + str(np.mean(postlosses))\n",
        "            print(print_str)\n",
        "            prelosses, postlosses = [], []\n",
        "\n",
        "        if (itr!=0) and itr % SAVE_INTERVAL == 0:\n",
        "            saver.save(sess, FLAGS.logdir + '/' + exp_string + '/model' + str(itr))\n",
        "\n",
        "        # sinusoid is infinite data, so no need to test on meta-validation set.\n",
        "        if (itr!=0) and itr % TEST_PRINT_INTERVAL == 0 and FLAGS.datasource !='sinusoid':\n",
        "            if 'generate' not in dir(data_generator):\n",
        "                feed_dict = {}\n",
        "                if model.classification:\n",
        "                    input_tensors = [model.metaval_total_accuracy1, model.metaval_total_accuracies2[FLAGS.num_updates-1], model.summ_op]\n",
        "                else:\n",
        "                    input_tensors = [model.metaval_total_loss1, model.metaval_total_losses2[FLAGS.num_updates-1], model.summ_op]\n",
        "            else:\n",
        "                batch_x, batch_y, amp, phase = data_generator.generate(train=False)\n",
        "                inputa = batch_x[:, :num_classes*FLAGS.update_batch_size, :]\n",
        "                inputb = batch_x[:, num_classes*FLAGS.update_batch_size:, :]\n",
        "                labela = batch_y[:, :num_classes*FLAGS.update_batch_size, :]\n",
        "                labelb = batch_y[:, num_classes*FLAGS.update_batch_size:, :]\n",
        "                feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb, model.meta_lr: 0.0}\n",
        "                if model.classification:\n",
        "                    input_tensors = [model.total_accuracy1, model.total_accuracies2[FLAGS.num_updates-1]]\n",
        "                else:\n",
        "                    input_tensors = [model.total_loss1, model.total_losses2[FLAGS.num_updates-1]]\n",
        "\n",
        "            result = sess.run(input_tensors, feed_dict)\n",
        "            print('Validation results: ' + str(result[0]) + ', ' + str(result[1]))\n",
        "\n",
        "    saver.save(sess, FLAGS.logdir + '/' + exp_string +  '/model' + str(itr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSLP895VQPU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculated for omniglot\n",
        "NUM_TEST_POINTS = 600"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDh1VAPRQP37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, saver, sess, exp_string, data_generator, test_num_updates=None):\n",
        "    num_classes = data_generator.num_classes # for classification, 1 otherwise\n",
        "\n",
        "    np.random.seed(1)\n",
        "    random.seed(1)\n",
        "\n",
        "    metaval_accuracies = []\n",
        "\n",
        "    for _ in range(NUM_TEST_POINTS):\n",
        "        if 'generate' not in dir(data_generator):\n",
        "            feed_dict = {}\n",
        "            feed_dict = {model.meta_lr : 0.0}\n",
        "        else:\n",
        "            batch_x, batch_y, amp, phase = data_generator.generate(train=False)\n",
        "\n",
        "            if FLAGS.baseline == 'oracle': # NOTE - this flag is specific to sinusoid\n",
        "                batch_x = np.concatenate([batch_x, np.zeros([batch_x.shape[0], batch_x.shape[1], 2])], 2)\n",
        "                batch_x[0, :, 1] = amp[0]\n",
        "                batch_x[0, :, 2] = phase[0]\n",
        "\n",
        "            inputa = batch_x[:, :num_classes*FLAGS.update_batch_size, :]\n",
        "            inputb = batch_x[:,num_classes*FLAGS.update_batch_size:, :]\n",
        "            labela = batch_y[:, :num_classes*FLAGS.update_batch_size, :]\n",
        "            labelb = batch_y[:,num_classes*FLAGS.update_batch_size:, :]\n",
        "\n",
        "            feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb, model.meta_lr: 0.0}\n",
        "\n",
        "        if model.classification:\n",
        "            result = sess.run([model.metaval_total_accuracy1] + model.metaval_total_accuracies2, feed_dict)\n",
        "        else:  # this is for sinusoid\n",
        "            result = sess.run([model.total_loss1] +  model.total_losses2, feed_dict)\n",
        "        metaval_accuracies.append(result)\n",
        "\n",
        "    metaval_accuracies = np.array(metaval_accuracies)\n",
        "    means = np.mean(metaval_accuracies, 0)\n",
        "    stds = np.std(metaval_accuracies, 0)\n",
        "    ci95 = 1.96*stds/np.sqrt(NUM_TEST_POINTS)\n",
        "\n",
        "    print('Mean validation accuracy/loss, stddev, and confidence intervals')\n",
        "    print((means, stds, ci95))\n",
        "\n",
        "    out_filename = FLAGS.logdir +'/'+ exp_string + '/' + 'test_ubs' + str(FLAGS.update_batch_size) + '_stepsize' + str(FLAGS.update_lr) + '.csv'\n",
        "    out_pkl = FLAGS.logdir +'/'+ exp_string + '/' + 'test_ubs' + str(FLAGS.update_batch_size) + '_stepsize' + str(FLAGS.update_lr) + '.pkl'\n",
        "    with open(out_pkl, 'wb') as f:\n",
        "        pickle.dump({'mses': metaval_accuracies}, f)\n",
        "    with open(out_filename, 'w') as f:\n",
        "        writer = csv.writer(f, delimiter=',')\n",
        "        writer.writerow(['update'+str(i) for i in range(len(means))])\n",
        "        writer.writerow(means)\n",
        "        writer.writerow(stds)\n",
        "        writer.writerow(ci95)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnKlW7IXQS5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  test_num_updates = 10\n",
        "\n",
        "  if FLAGS.train == False:\n",
        "      orig_meta_batch_size = FLAGS.meta_batch_size\n",
        "      # always use meta batch size of 1 when testing.\n",
        "      FLAGS.meta_batch_size = 1\n",
        "\n",
        "  if FLAGS.datasource == 'sinusoid':\n",
        "      data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)\n",
        "  else:\n",
        "      if FLAGS.metatrain_iterations == 0 and FLAGS.datasource == 'miniimagenet':\n",
        "          assert FLAGS.meta_batch_size == 1\n",
        "          assert FLAGS.update_batch_size == 1\n",
        "          data_generator = DataGenerator(1, FLAGS.meta_batch_size)  # only use one datapoint,\n",
        "      else:\n",
        "          if FLAGS.datasource == 'miniimagenet': # TODO - use 15 val examples for imagenet?\n",
        "              if FLAGS.train:\n",
        "                  data_generator = DataGenerator(FLAGS.update_batch_size+15, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
        "              else:\n",
        "                  data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
        "          else:\n",
        "              data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
        "\n",
        "\n",
        "  dim_output = data_generator.dim_output\n",
        "  if FLAGS.baseline == 'oracle':\n",
        "      assert FLAGS.datasource == 'sinusoid'\n",
        "      dim_input = 3\n",
        "      FLAGS.pretrain_iterations += FLAGS.metatrain_iterations\n",
        "      FLAGS.metatrain_iterations = 0\n",
        "  else:\n",
        "      dim_input = data_generator.dim_input\n",
        "\n",
        "  if FLAGS.datasource == 'miniimagenet' or FLAGS.datasource == 'omniglot':\n",
        "      tf_data_load = True\n",
        "      num_classes = data_generator.num_classes\n",
        "\n",
        "      if FLAGS.train: # only construct training model if needed\n",
        "          random.seed(5)\n",
        "          image_tensor, label_tensor = data_generator.make_data_tensor()\n",
        "          inputa = tf.slice(image_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
        "          inputb = tf.slice(image_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
        "          labela = tf.slice(label_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
        "          labelb = tf.slice(label_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
        "          input_tensors = {'inputa': inputa, 'inputb': inputb, 'labela': labela, 'labelb': labelb}\n",
        "\n",
        "      random.seed(6)\n",
        "      image_tensor, label_tensor = data_generator.make_data_tensor(train=False)\n",
        "      inputa = tf.slice(image_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
        "      inputb = tf.slice(image_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
        "      labela = tf.slice(label_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
        "      labelb = tf.slice(label_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
        "      metaval_input_tensors = {'inputa': inputa, 'inputb': inputb, 'labela': labela, 'labelb': labelb}\n",
        "  else:\n",
        "      tf_data_load = False\n",
        "      input_tensors = None\n",
        "\n",
        "  model = MAML(dim_input, dim_output, test_num_updates=test_num_updates)\n",
        "  if FLAGS.train or not tf_data_load:\n",
        "      model.construct_model(input_tensors=input_tensors, prefix='metatrain_')\n",
        "  if tf_data_load:\n",
        "      model.construct_model(input_tensors=metaval_input_tensors, prefix='metaval_')\n",
        "  model.summ_op = tf.summary.merge_all()\n",
        "\n",
        "  saver = loader = tf.train.Saver(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES), max_to_keep=10)\n",
        "\n",
        "  sess = tf.InteractiveSession()\n",
        "\n",
        "  if FLAGS.train == False:\n",
        "      # change to original meta batch size when loading model.\n",
        "      FLAGS.meta_batch_size = orig_meta_batch_size\n",
        "\n",
        "  if FLAGS.train_update_batch_size == -1:\n",
        "      FLAGS.train_update_batch_size = FLAGS.update_batch_size\n",
        "  if FLAGS.train_update_lr == -1:\n",
        "      FLAGS.train_update_lr = FLAGS.update_lr\n",
        "\n",
        "  exp_string = 'cls_'+str(FLAGS.num_classes)+'.mbs_'+str(FLAGS.meta_batch_size) + '.ubs_' + str(FLAGS.train_update_batch_size) + '.numstep' + str(FLAGS.num_updates) + '.updatelr' + str(FLAGS.train_update_lr)\n",
        "\n",
        "  if FLAGS.num_filters != 64:\n",
        "      exp_string += 'hidden' + str(FLAGS.num_filters)\n",
        "  if FLAGS.max_pool:\n",
        "      exp_string += 'maxpool'\n",
        "  if FLAGS.stop_grad:\n",
        "      exp_string += 'stopgrad'\n",
        "  if FLAGS.baseline:\n",
        "      exp_string += FLAGS.baseline\n",
        "  if FLAGS.norm == 'batch_norm':\n",
        "      exp_string += 'batchnorm'\n",
        "  elif FLAGS.norm == 'layer_norm':\n",
        "      exp_string += 'layernorm'\n",
        "  elif FLAGS.norm == 'None':\n",
        "      exp_string += 'nonorm'\n",
        "  else:\n",
        "      print('Norm setting not recognized.')\n",
        "\n",
        "  resume_itr = 0\n",
        "  model_file = None\n",
        "\n",
        "  tf.global_variables_initializer().run()\n",
        "  tf.train.start_queue_runners()\n",
        "\n",
        "  if FLAGS.resume or not FLAGS.train:\n",
        "      model_file = tf.train.latest_checkpoint(FLAGS.logdir + '/' + exp_string)\n",
        "      if FLAGS.test_iter > 0:\n",
        "          model_file = model_file[:model_file.index('model')] + 'model' + str(FLAGS.test_iter)\n",
        "      if model_file:\n",
        "          ind1 = model_file.index('model')\n",
        "          resume_itr = int(model_file[ind1+5:])\n",
        "          print(\"Restoring model weights from \" + model_file)\n",
        "          saver.restore(sess, model_file)\n",
        "\n",
        "  if FLAGS.train:\n",
        "      train(model, saver, sess, exp_string, data_generator, resume_itr)\n",
        "  else:\n",
        "      test(model, saver, sess, exp_string, data_generator, test_num_updates)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syxmqOsJQXLs",
        "colab_type": "code",
        "outputId": "9362f126-541b-4bdc-dd66-3bd881328807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  FLAGS.datasource='omniglot' \n",
        "  FLAGS.metatrain_iterations=60000 \n",
        "  FLAGS.meta_batch_size=32 \n",
        "  FLAGS.update_batch_size=1 \n",
        "  FLAGS.update_lr=0.4 \n",
        "  FLAGS.num_updates=1 \n",
        "  FLAGS.logdir='logs/omniglot5way/'\n",
        "  main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating filenames\n",
            "Generating image processing ops\n",
            "Batching images\n",
            "Manipulating image data to be right shape\n",
            "Generating filenames\n",
            "Generating image processing ops\n",
            "Batching images\n",
            "Manipulating image data to be right shape\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0702 08:54:07.145343 140344355174272 deprecation.py:323] From <ipython-input-10-4b787753a554>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W0702 08:54:15.883780 140344355174272 deprecation.py:323] From <ipython-input-19-77db55fb89df>:108: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0702 08:54:32.607943 140344355174272 deprecation.py:323] From <ipython-input-17-1bfb9dc15133>:102: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done initializing, starting training.\n",
            "Iteration 100: 0.20625, 0.29375\n",
            "Iteration 200: 0.21875, 0.5\n",
            "Iteration 300: 0.2125, 0.59375\n",
            "Iteration 400: 0.16875, 0.55\n",
            "Iteration 500: 0.24374999, 0.54375005\n",
            "Validation results: 0.21875, 0.75625\n",
            "Iteration 600: 0.25, 0.75\n",
            "Iteration 700: 0.20625001, 0.66875005\n",
            "Iteration 800: 0.23125, 0.63125\n",
            "Iteration 900: 0.24375, 0.76250005\n",
            "Iteration 1000: 0.2, 0.7375\n",
            "Validation results: 0.16250001, 0.78125\n",
            "Iteration 1100: 0.21875, 0.70000005\n",
            "Iteration 1200: 0.21875, 0.78125\n",
            "Iteration 1300: 0.2375, 0.63125\n",
            "Iteration 1400: 0.25, 0.75625\n",
            "Iteration 1500: 0.14375001, 0.75\n",
            "Validation results: 0.14375001, 0.83125\n",
            "Iteration 1600: 0.20000002, 0.8375\n",
            "Iteration 1700: 0.29375, 0.775\n",
            "Iteration 1800: 0.19375, 0.84375\n",
            "Iteration 1900: 0.21875, 0.84375\n",
            "Iteration 2000: 0.19375, 0.83125\n",
            "Validation results: 0.17500001, 0.86875\n",
            "Iteration 2100: 0.20625001, 0.81875\n",
            "Iteration 2200: 0.15, 0.8375\n",
            "Iteration 2300: 0.25, 0.90625\n",
            "Iteration 2400: 0.2, 0.83125\n",
            "Iteration 2500: 0.22500001, 0.86875\n",
            "Validation results: 0.15625, 0.85625\n",
            "Iteration 2600: 0.1875, 0.8625\n",
            "Iteration 2700: 0.19375, 0.89374995\n",
            "Iteration 2800: 0.2125, 0.90625\n",
            "Iteration 2900: 0.17500001, 0.81875\n",
            "Iteration 3000: 0.23124999, 0.86249995\n",
            "Validation results: 0.125, 0.9125\n",
            "Iteration 3100: 0.19375, 0.84375\n",
            "Iteration 3200: 0.20625001, 0.90625\n",
            "Iteration 3300: 0.15, 0.85\n",
            "Iteration 3400: 0.20625001, 0.8875\n",
            "Iteration 3500: 0.18125, 0.88125\n",
            "Validation results: 0.20625001, 0.93125\n",
            "Iteration 3600: 0.1875, 0.8875\n",
            "Iteration 3700: 0.225, 0.925\n",
            "Iteration 3800: 0.20625001, 0.91875\n",
            "Iteration 3900: 0.1875, 0.89375\n",
            "Iteration 4000: 0.1875, 0.88124996\n",
            "Validation results: 0.2125, 0.925\n",
            "Iteration 4100: 0.23125, 0.93125\n",
            "Iteration 4200: 0.16875, 0.91249996\n",
            "Iteration 4300: 0.29375, 0.9\n",
            "Iteration 4400: 0.19375001, 0.90625\n",
            "Iteration 4500: 0.20625001, 0.91875\n",
            "Validation results: 0.1875, 0.93125\n",
            "Iteration 4600: 0.21875, 0.8375\n",
            "Iteration 4700: 0.1375, 0.91875\n",
            "Iteration 4800: 0.1625, 0.925\n",
            "Iteration 4900: 0.20625001, 0.93125\n",
            "Iteration 5000: 0.175, 0.9375\n",
            "Validation results: 0.1875, 0.90625\n",
            "Iteration 5100: 0.21875, 0.94375\n",
            "Iteration 5200: 0.21875, 0.9\n",
            "Iteration 5300: 0.23125, 0.9375\n",
            "Iteration 5400: 0.13749999, 0.89375\n",
            "Iteration 5500: 0.19375001, 0.9375\n",
            "Validation results: 0.1875, 0.95\n",
            "Iteration 5600: 0.2125, 0.91875\n",
            "Iteration 5700: 0.1625, 0.93125\n",
            "Iteration 5800: 0.24375, 0.95625\n",
            "Iteration 5900: 0.21875, 0.92499995\n",
            "Iteration 6000: 0.18125, 0.93125\n",
            "Validation results: 0.23125002, 0.92499995\n",
            "Iteration 6100: 0.2125, 0.975\n",
            "Iteration 6200: 0.2125, 0.92499995\n",
            "Iteration 6300: 0.23750001, 0.94375\n",
            "Iteration 6400: 0.19375001, 0.93125\n",
            "Iteration 6500: 0.1875, 0.95625\n",
            "Validation results: 0.19375, 0.95000005\n",
            "Iteration 6600: 0.1875, 0.98125005\n",
            "Iteration 6700: 0.23125, 0.93125\n",
            "Iteration 6800: 0.20625001, 0.93125\n",
            "Iteration 6900: 0.23125, 0.9625\n",
            "Iteration 7000: 0.20625, 0.9375\n",
            "Validation results: 0.19375, 0.93125\n",
            "Iteration 7100: 0.21875, 0.95\n",
            "Iteration 7200: 0.22500001, 0.94375\n",
            "Iteration 7300: 0.20000002, 0.95\n",
            "Iteration 7400: 0.2, 0.96875\n",
            "Iteration 7500: 0.18125, 0.9375\n",
            "Validation results: 0.18125, 0.96875\n",
            "Iteration 7600: 0.2375, 0.9625\n",
            "Iteration 7700: 0.225, 0.95624995\n",
            "Iteration 7800: 0.19375001, 0.95\n",
            "Iteration 7900: 0.2375, 0.9625\n",
            "Iteration 8000: 0.19375, 0.9625\n",
            "Validation results: 0.19375, 0.95625\n",
            "Iteration 8100: 0.16875, 0.95624995\n",
            "Iteration 8200: 0.15625, 0.96875\n",
            "Iteration 8300: 0.2, 0.94375\n",
            "Iteration 8400: 0.1875, 0.94375\n",
            "Iteration 8500: 0.2125, 0.92499995\n",
            "Validation results: 0.2125, 0.96875\n",
            "Iteration 8600: 0.25625, 0.96875\n",
            "Iteration 8700: 0.20625, 0.96875\n",
            "Iteration 8800: 0.17500001, 0.9375\n",
            "Iteration 8900: 0.20625001, 0.9625\n",
            "Iteration 9000: 0.20625001, 0.95625\n",
            "Validation results: 0.2, 0.95625\n",
            "Iteration 9100: 0.15, 0.98125005\n",
            "Iteration 9200: 0.2125, 0.9625\n",
            "Iteration 9300: 0.1875, 0.96875\n",
            "Iteration 9400: 0.20625, 0.91875005\n",
            "Iteration 9500: 0.17500001, 0.95000005\n",
            "Validation results: 0.21875001, 0.91875\n",
            "Iteration 9600: 0.17500001, 0.975\n",
            "Iteration 9700: 0.19375, 0.95624995\n",
            "Iteration 9800: 0.2, 0.975\n",
            "Iteration 9900: 0.20625, 0.95\n",
            "Iteration 10000: 0.19375, 0.95624995\n",
            "Validation results: 0.18125, 0.9625\n",
            "Iteration 10100: 0.18125, 0.925\n",
            "Iteration 10200: 0.13125, 0.975\n",
            "Iteration 10300: 0.2125, 0.95625\n",
            "Iteration 10400: 0.15, 0.975\n",
            "Iteration 10500: 0.19999999, 0.94375\n",
            "Validation results: 0.26875, 0.975\n",
            "Iteration 10600: 0.20625, 0.9625\n",
            "Iteration 10700: 0.19375001, 0.98125\n",
            "Iteration 10800: 0.175, 0.98125005\n",
            "Iteration 10900: 0.16250001, 0.96875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0702 10:20:56.908564 140344355174272 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 11000: 0.2125, 0.9625\n",
            "Validation results: 0.23750001, 0.9375\n",
            "Iteration 11100: 0.21875, 0.96250004\n",
            "Iteration 11200: 0.21875, 0.975\n",
            "Iteration 11300: 0.21875, 0.95625\n",
            "Iteration 11400: 0.17500001, 0.98125005\n",
            "Iteration 11500: 0.225, 0.9875\n",
            "Validation results: 0.18125, 0.9875\n",
            "Iteration 11600: 0.175, 0.91875005\n",
            "Iteration 11700: 0.19375, 0.96875\n",
            "Iteration 11800: 0.21875, 0.9625\n",
            "Iteration 11900: 0.16875002, 0.95624995\n",
            "Iteration 12000: 0.175, 0.96875\n",
            "Validation results: 0.225, 0.96875\n",
            "Iteration 12100: 0.22500001, 0.9875\n",
            "Iteration 12200: 0.2375, 0.9625\n",
            "Iteration 12300: 0.16875, 0.975\n",
            "Iteration 12400: 0.23750001, 0.98125\n",
            "Iteration 12500: 0.21875, 0.96875\n",
            "Validation results: 0.112500004, 0.96875\n",
            "Iteration 12600: 0.1625, 0.96875\n",
            "Iteration 12700: 0.14375001, 0.95\n",
            "Iteration 12800: 0.15625, 0.975\n",
            "Iteration 12900: 0.25625, 0.975\n",
            "Iteration 13000: 0.13749999, 0.98125005\n",
            "Validation results: 0.2125, 0.9875\n",
            "Iteration 13100: 0.19999999, 0.9875\n",
            "Iteration 13200: 0.21875, 0.98125005\n",
            "Iteration 13300: 0.1875, 0.975\n",
            "Iteration 13400: 0.21875, 0.99375\n",
            "Iteration 13500: 0.2125, 0.98125005\n",
            "Validation results: 0.16875, 0.98125005\n",
            "Iteration 13600: 0.20625001, 0.9875\n",
            "Iteration 13700: 0.175, 0.9875\n",
            "Iteration 13800: 0.15625, 0.975\n",
            "Iteration 13900: 0.17500001, 0.975\n",
            "Iteration 14000: 0.22500001, 0.99375\n",
            "Validation results: 0.15, 0.9875\n",
            "Iteration 14100: 0.14375001, 0.975\n",
            "Iteration 14200: 0.18125, 0.94374996\n",
            "Iteration 14300: 0.175, 0.94375\n",
            "Iteration 14400: 0.15, 0.94375\n",
            "Iteration 14500: 0.2, 0.96875\n",
            "Validation results: 0.19375001, 0.98125005\n",
            "Iteration 14600: 0.20000002, 0.98125\n",
            "Iteration 14700: 0.22500001, 0.9875\n",
            "Iteration 14800: 0.16875, 0.975\n",
            "Iteration 14900: 0.19375, 0.9375\n",
            "Iteration 15000: 0.16875, 0.975\n",
            "Validation results: 0.18125, 0.96250004\n",
            "Iteration 15100: 0.28125, 0.99375\n",
            "Iteration 15200: 0.2, 0.975\n",
            "Iteration 15300: 0.2, 0.98125005\n",
            "Iteration 15400: 0.225, 0.9875\n",
            "Iteration 15500: 0.1875, 0.98125005\n",
            "Validation results: 0.225, 1.0\n",
            "Iteration 15600: 0.16250001, 1.0\n",
            "Iteration 15700: 0.23125002, 0.94375\n",
            "Iteration 15800: 0.1875, 0.98125\n",
            "Iteration 15900: 0.20625, 0.9875\n",
            "Iteration 16000: 0.19375, 0.98125005\n",
            "Validation results: 0.16875, 0.96875\n",
            "Iteration 16100: 0.20625001, 0.95624995\n",
            "Iteration 16200: 0.2125, 0.975\n",
            "Iteration 16300: 0.10625, 0.95624995\n",
            "Iteration 16400: 0.2125, 0.95625\n",
            "Iteration 16500: 0.1875, 0.9625\n",
            "Validation results: 0.25, 0.96875\n",
            "Iteration 16600: 0.25, 0.9875\n",
            "Iteration 16700: 0.18125, 0.96875\n",
            "Iteration 16800: 0.19999999, 0.975\n",
            "Iteration 16900: 0.2, 0.99375\n",
            "Iteration 17000: 0.2, 0.975\n",
            "Validation results: 0.2125, 0.9875\n",
            "Iteration 17100: 0.21875, 0.98125005\n",
            "Iteration 17200: 0.20625, 0.95625\n",
            "Iteration 17300: 0.21875, 0.9875\n",
            "Iteration 17400: 0.1875, 0.9875\n",
            "Iteration 17500: 0.17500001, 0.975\n",
            "Validation results: 0.20000002, 0.9625\n",
            "Iteration 17600: 0.25, 0.96875\n",
            "Iteration 17700: 0.20625, 0.98125005\n",
            "Iteration 17800: 0.15, 0.9875\n",
            "Iteration 17900: 0.2125, 0.98125\n",
            "Iteration 18000: 0.22500001, 0.975\n",
            "Validation results: 0.23125, 1.0\n",
            "Iteration 18100: 0.19375, 0.975\n",
            "Iteration 18200: 0.17500001, 0.9625\n",
            "Iteration 18300: 0.22500001, 0.9875\n",
            "Iteration 18400: 0.21875001, 0.99375\n",
            "Iteration 18500: 0.21875, 0.9875\n",
            "Validation results: 0.16875, 0.94375\n",
            "Iteration 18600: 0.2, 0.99375\n",
            "Iteration 18700: 0.18125, 0.98125\n",
            "Iteration 18800: 0.24375, 0.9625\n",
            "Iteration 18900: 0.1875, 0.99375\n",
            "Iteration 19000: 0.18125, 0.9875\n",
            "Validation results: 0.17500001, 0.99375\n",
            "Iteration 19100: 0.13125001, 0.98125005\n",
            "Iteration 19200: 0.2, 0.975\n",
            "Iteration 19300: 0.20625, 0.975\n",
            "Iteration 19400: 0.18125, 0.9875\n",
            "Iteration 19500: 0.19375, 1.0\n",
            "Validation results: 0.24374999, 0.98125005\n",
            "Iteration 19600: 0.21875, 0.9875\n",
            "Iteration 19700: 0.19375, 0.96875\n",
            "Iteration 19800: 0.21249999, 0.98125005\n",
            "Iteration 19900: 0.20625, 0.9875\n",
            "Iteration 20000: 0.20625001, 0.975\n",
            "Validation results: 0.23125, 0.99375\n",
            "Iteration 20100: 0.25, 0.98125005\n",
            "Iteration 20200: 0.24375, 0.99375\n",
            "Iteration 20300: 0.19375, 0.95624995\n",
            "Iteration 20400: 0.19375001, 0.98125005\n",
            "Iteration 20500: 0.2125, 0.9875\n",
            "Validation results: 0.1875, 0.99375\n",
            "Iteration 20600: 0.21875, 0.96875\n",
            "Iteration 20700: 0.16875, 0.975\n",
            "Iteration 20800: 0.17500001, 0.9875\n",
            "Iteration 20900: 0.21875, 0.975\n",
            "Iteration 21000: 0.21875, 0.9875\n",
            "Validation results: 0.2, 0.9875\n",
            "Iteration 21100: 0.24375, 0.9875\n",
            "Iteration 21200: 0.23125, 1.0\n",
            "Iteration 21300: 0.16875, 0.99375\n",
            "Iteration 21400: 0.17500001, 0.99375\n",
            "Iteration 21500: 0.20625001, 0.96875\n",
            "Validation results: 0.16875, 0.98125005\n",
            "Iteration 21600: 0.18125, 0.9875\n",
            "Iteration 21700: 0.23125, 0.9875\n",
            "Iteration 21800: 0.20625001, 0.98125005\n",
            "Iteration 21900: 0.20625001, 0.98125005\n",
            "Iteration 22000: 0.15, 0.99375\n",
            "Validation results: 0.26875, 0.975\n",
            "Iteration 22100: 0.1625, 0.9875\n",
            "Iteration 22200: 0.22500001, 0.99375\n",
            "Iteration 22300: 0.19999999, 0.9875\n",
            "Iteration 22400: 0.20625, 0.99375\n",
            "Iteration 22500: 0.24375, 0.9875\n",
            "Validation results: 0.24375, 0.975\n",
            "Iteration 22600: 0.18125, 1.0\n",
            "Iteration 22700: 0.16874999, 0.975\n",
            "Iteration 22800: 0.19375, 0.9875\n",
            "Iteration 22900: 0.20625, 0.99375\n",
            "Iteration 23000: 0.19375, 0.975\n",
            "Validation results: 0.1625, 0.96875\n",
            "Iteration 23100: 0.15625, 1.0\n",
            "Iteration 23200: 0.19375001, 0.99375\n",
            "Iteration 23300: 0.20625, 0.9875\n",
            "Iteration 23400: 0.2, 0.98125\n",
            "Iteration 23500: 0.24375, 0.98125\n",
            "Validation results: 0.22500001, 1.0\n",
            "Iteration 23600: 0.16875, 0.98125005\n",
            "Iteration 23700: 0.1875, 0.975\n",
            "Iteration 23800: 0.2125, 0.99375\n",
            "Iteration 23900: 0.16250001, 0.9875\n",
            "Iteration 24000: 0.15625, 0.96875\n",
            "Validation results: 0.17500001, 0.96875\n",
            "Iteration 24100: 0.225, 0.9875\n",
            "Iteration 24200: 0.21875, 0.99375\n",
            "Iteration 24300: 0.19375, 0.9875\n",
            "Iteration 24400: 0.22500001, 0.99375\n",
            "Iteration 24500: 0.2375, 0.96875\n",
            "Validation results: 0.1875, 0.99375\n",
            "Iteration 24600: 0.2, 0.99375\n",
            "Iteration 24700: 0.15625, 0.95\n",
            "Iteration 24800: 0.225, 0.9875\n",
            "Iteration 24900: 0.23125, 1.0\n",
            "Iteration 25000: 0.21875, 0.99375\n",
            "Validation results: 0.17500001, 0.98125005\n",
            "Iteration 25100: 0.20625, 0.975\n",
            "Iteration 25200: 0.2125, 0.9875\n",
            "Iteration 25300: 0.18125, 0.99375\n",
            "Iteration 25400: 0.14375, 0.9375\n",
            "Iteration 25500: 0.2, 0.98125005\n",
            "Validation results: 0.1875, 0.99375\n",
            "Iteration 25600: 0.2125, 0.975\n",
            "Iteration 25700: 0.1875, 0.9875\n",
            "Iteration 25800: 0.15, 0.9875\n",
            "Iteration 25900: 0.16875002, 0.9875\n",
            "Iteration 26000: 0.16875, 0.975\n",
            "Validation results: 0.19375001, 0.975\n",
            "Iteration 26100: 0.1875, 0.9875\n",
            "Iteration 26200: 0.1875, 0.9875\n",
            "Iteration 26300: 0.19375, 0.9875\n",
            "Iteration 26400: 0.18125, 1.0\n",
            "Iteration 26500: 0.19999999, 1.0\n",
            "Validation results: 0.225, 0.98125005\n",
            "Iteration 26600: 0.23125, 0.99375\n",
            "Iteration 26700: 0.26875, 0.9875\n",
            "Iteration 26800: 0.2125, 0.99375\n",
            "Iteration 26900: 0.23125, 0.9875\n",
            "Iteration 27000: 0.20625, 0.98125005\n",
            "Validation results: 0.1375, 0.99375\n",
            "Iteration 27100: 0.1625, 0.98125005\n",
            "Iteration 27200: 0.19375, 0.99375\n",
            "Iteration 27300: 0.18125, 0.98125005\n",
            "Iteration 27400: 0.23125002, 0.98125\n",
            "Iteration 27500: 0.24375, 0.9625\n",
            "Validation results: 0.175, 0.975\n",
            "Iteration 27600: 0.2125, 0.9875\n",
            "Iteration 27700: 0.24374999, 0.98125005\n",
            "Iteration 27800: 0.19375001, 0.98125005\n",
            "Iteration 27900: 0.17500001, 0.9875\n",
            "Iteration 28000: 0.19375001, 0.975\n",
            "Validation results: 0.16875, 0.9875\n",
            "Iteration 28100: 0.21875, 0.9875\n",
            "Iteration 28200: 0.1875, 0.975\n",
            "Iteration 28300: 0.175, 0.9875\n",
            "Iteration 28400: 0.20000002, 1.0\n",
            "Iteration 28500: 0.1125, 0.98125005\n",
            "Validation results: 0.20625001, 0.99375\n",
            "Iteration 28600: 0.21875001, 0.98125005\n",
            "Iteration 28700: 0.18125, 0.99375\n",
            "Iteration 28800: 0.26250002, 0.9875\n",
            "Iteration 28900: 0.17500001, 0.99375\n",
            "Iteration 29000: 0.18125, 1.0\n",
            "Validation results: 0.175, 0.96875\n",
            "Iteration 29100: 0.175, 1.0\n",
            "Iteration 29200: 0.2125, 0.96875\n",
            "Iteration 29300: 0.2, 0.9875\n",
            "Iteration 29400: 0.2375, 0.99375\n",
            "Iteration 29500: 0.18125, 0.975\n",
            "Validation results: 0.1625, 0.99375\n",
            "Iteration 29600: 0.1875, 0.98125005\n",
            "Iteration 29700: 0.2375, 1.0\n",
            "Iteration 29800: 0.175, 0.99375\n",
            "Iteration 29900: 0.24375, 0.9875\n",
            "Iteration 30000: 0.1875, 0.975\n",
            "Validation results: 0.18125, 0.975\n",
            "Iteration 30100: 0.2125, 0.98125005\n",
            "Iteration 30200: 0.19375001, 0.99375\n",
            "Iteration 30300: 0.1875, 0.9875\n",
            "Iteration 30400: 0.15625, 0.9875\n",
            "Iteration 30500: 0.16250001, 0.9875\n",
            "Validation results: 0.2375, 0.99375\n",
            "Iteration 30600: 0.225, 1.0\n",
            "Iteration 30700: 0.20625001, 0.9875\n",
            "Iteration 30800: 0.1875, 0.99375\n",
            "Iteration 30900: 0.20000002, 0.9875\n",
            "Iteration 31000: 0.20625001, 0.9875\n",
            "Validation results: 0.1875, 0.98125\n",
            "Iteration 31100: 0.25625, 0.98125005\n",
            "Iteration 31200: 0.15, 0.99375\n",
            "Iteration 31300: 0.24374999, 0.9875\n",
            "Iteration 31400: 0.22500001, 0.99375\n",
            "Iteration 31500: 0.1625, 0.98125005\n",
            "Validation results: 0.118750006, 0.9875\n",
            "Iteration 31600: 0.23125, 0.975\n",
            "Iteration 31700: 0.21875, 0.9875\n",
            "Iteration 31800: 0.20625001, 0.99375\n",
            "Iteration 31900: 0.15625, 0.99375\n",
            "Iteration 32000: 0.23750001, 0.9875\n",
            "Validation results: 0.16250001, 0.99375\n",
            "Iteration 32100: 0.15, 0.975\n",
            "Iteration 32200: 0.18125, 0.98125005\n",
            "Iteration 32300: 0.1875, 0.99375\n",
            "Iteration 32400: 0.16875, 0.99375\n",
            "Iteration 32500: 0.23125, 0.9875\n",
            "Validation results: 0.20625001, 0.98125005\n",
            "Iteration 32600: 0.2125, 0.9875\n",
            "Iteration 32700: 0.2125, 0.98125\n",
            "Iteration 32800: 0.2, 0.9625\n",
            "Iteration 32900: 0.1875, 0.99375\n",
            "Iteration 33000: 0.15, 0.99375\n",
            "Validation results: 0.2, 0.9875\n",
            "Iteration 33100: 0.1625, 0.99375\n",
            "Iteration 33200: 0.125, 0.99375\n",
            "Iteration 33300: 0.25625002, 0.9875\n",
            "Iteration 33400: 0.2125, 0.9875\n",
            "Iteration 33500: 0.20625001, 0.9875\n",
            "Validation results: 0.19375001, 1.0\n",
            "Iteration 33600: 0.15625, 0.99375\n",
            "Iteration 33700: 0.20625001, 0.99375\n",
            "Iteration 33800: 0.175, 0.98125005\n",
            "Iteration 33900: 0.20000002, 0.98125005\n",
            "Iteration 34000: 0.1875, 1.0\n",
            "Validation results: 0.15625, 0.975\n",
            "Iteration 34100: 0.23750001, 0.98125005\n",
            "Iteration 34200: 0.225, 0.99375\n",
            "Iteration 34300: 0.21875, 0.99375\n",
            "Iteration 34400: 0.16875002, 0.99375\n",
            "Iteration 34500: 0.20625001, 1.0\n",
            "Validation results: 0.21875, 0.98125005\n",
            "Iteration 34600: 0.23750001, 0.98125005\n",
            "Iteration 34700: 0.19375, 1.0\n",
            "Iteration 34800: 0.19375001, 0.9875\n",
            "Iteration 34900: 0.15625, 0.99375\n",
            "Iteration 35000: 0.18125, 0.99375\n",
            "Validation results: 0.225, 0.99375\n",
            "Iteration 35100: 0.20625001, 1.0\n",
            "Iteration 35200: 0.15625, 0.98125005\n",
            "Iteration 35300: 0.18125, 0.975\n",
            "Iteration 35400: 0.2, 0.99375\n",
            "Iteration 35500: 0.2375, 0.9875\n",
            "Validation results: 0.16875, 0.98125005\n",
            "Iteration 35600: 0.19999999, 0.975\n",
            "Iteration 35700: 0.19375001, 0.99375\n",
            "Iteration 35800: 0.10625, 0.9875\n",
            "Iteration 35900: 0.1875, 0.99375\n",
            "Iteration 36000: 0.18125, 1.0\n",
            "Validation results: 0.19999999, 0.99375\n",
            "Iteration 36100: 0.21875, 0.9875\n",
            "Iteration 36200: 0.18125, 0.9875\n",
            "Iteration 36300: 0.1875, 0.98125005\n",
            "Iteration 36400: 0.18125, 0.98125005\n",
            "Iteration 36500: 0.2125, 0.99375\n",
            "Validation results: 0.1875, 0.98125\n",
            "Iteration 36600: 0.2375, 0.9875\n",
            "Iteration 36700: 0.1625, 0.99375\n",
            "Iteration 36800: 0.2625, 0.96875\n",
            "Iteration 36900: 0.16250001, 0.98125\n",
            "Iteration 37000: 0.15625, 0.99375\n",
            "Validation results: 0.15625, 0.99375\n",
            "Iteration 37100: 0.19375001, 0.98125\n",
            "Iteration 37200: 0.23750001, 0.99375\n",
            "Iteration 37300: 0.18750001, 0.99375\n",
            "Iteration 37400: 0.19999999, 0.9875\n",
            "Iteration 37500: 0.23124999, 0.98125005\n",
            "Validation results: 0.17500001, 0.99375\n",
            "Iteration 37600: 0.23750001, 0.99375\n",
            "Iteration 37700: 0.23124999, 0.9875\n",
            "Iteration 37800: 0.2125, 1.0\n",
            "Iteration 37900: 0.21875, 0.9875\n",
            "Iteration 38000: 0.1875, 0.99375\n",
            "Validation results: 0.175, 0.9875\n",
            "Iteration 38100: 0.2, 0.99375\n",
            "Iteration 38200: 0.15, 0.99375\n",
            "Iteration 38300: 0.23750001, 0.96875\n",
            "Iteration 38400: 0.225, 0.99375\n",
            "Iteration 38500: 0.19375, 0.9875\n",
            "Validation results: 0.2, 0.975\n",
            "Iteration 38600: 0.225, 0.98125005\n",
            "Iteration 38700: 0.18125, 1.0\n",
            "Iteration 38800: 0.16250001, 0.96875\n",
            "Iteration 38900: 0.21875, 0.9875\n",
            "Iteration 39000: 0.25, 0.9625\n",
            "Validation results: 0.19375, 0.98125\n",
            "Iteration 39100: 0.13125, 0.9875\n",
            "Iteration 39200: 0.2125, 0.98125005\n",
            "Iteration 39300: 0.2125, 1.0\n",
            "Iteration 39400: 0.2, 0.99375\n",
            "Iteration 39500: 0.19375001, 0.99375\n",
            "Validation results: 0.21875, 0.9875\n",
            "Iteration 39600: 0.18125, 0.99375\n",
            "Iteration 39700: 0.23125, 0.99375\n",
            "Iteration 39800: 0.16875, 0.98125\n",
            "Iteration 39900: 0.15625, 1.0\n",
            "Iteration 40000: 0.23750001, 0.96875\n",
            "Validation results: 0.20625001, 0.99375\n",
            "Iteration 40100: 0.18125, 0.99375\n",
            "Iteration 40200: 0.23124999, 0.99375\n",
            "Iteration 40300: 0.16875, 0.9875\n",
            "Iteration 40400: 0.26875, 0.98125005\n",
            "Iteration 40500: 0.1875, 0.99375\n",
            "Validation results: 0.16875, 1.0\n",
            "Iteration 40600: 0.15, 0.9875\n",
            "Iteration 40700: 0.24375, 1.0\n",
            "Iteration 40800: 0.23125002, 0.99375\n",
            "Iteration 40900: 0.2125, 0.9875\n",
            "Iteration 41000: 0.16875002, 0.9875\n",
            "Validation results: 0.17500001, 0.98125005\n",
            "Iteration 41100: 0.24375, 1.0\n",
            "Iteration 41200: 0.18125, 0.98125005\n",
            "Iteration 41300: 0.225, 0.9875\n",
            "Iteration 41400: 0.19375001, 0.99375\n",
            "Iteration 41500: 0.2125, 0.99375\n",
            "Validation results: 0.225, 0.99375\n",
            "Iteration 41600: 0.24375, 0.9875\n",
            "Iteration 41700: 0.23125, 0.98125\n",
            "Iteration 41800: 0.24375, 0.98125005\n",
            "Iteration 41900: 0.18125, 0.98125\n",
            "Iteration 42000: 0.14375001, 0.975\n",
            "Validation results: 0.1875, 0.99375\n",
            "Iteration 42100: 0.2, 0.98125005\n",
            "Iteration 42200: 0.1875, 0.98125005\n",
            "Iteration 42300: 0.23125002, 0.96875\n",
            "Iteration 42400: 0.20625001, 0.99375\n",
            "Iteration 42500: 0.23125002, 0.99375\n",
            "Validation results: 0.16875002, 0.98125005\n",
            "Iteration 42600: 0.18125, 0.99375\n",
            "Iteration 42700: 0.1875, 1.0\n",
            "Iteration 42800: 0.24375, 0.9625\n",
            "Iteration 42900: 0.1625, 0.98125\n",
            "Iteration 43000: 0.2125, 0.99375\n",
            "Validation results: 0.16250001, 0.99375\n",
            "Iteration 43100: 0.16250001, 0.99375\n",
            "Iteration 43200: 0.21875, 0.9875\n",
            "Iteration 43300: 0.24375, 0.99375\n",
            "Iteration 43400: 0.19375, 1.0\n",
            "Iteration 43500: 0.15625, 0.9875\n",
            "Validation results: 0.118750006, 0.99375\n",
            "Iteration 43600: 0.26875, 1.0\n",
            "Iteration 43700: 0.19375, 0.99375\n",
            "Iteration 43800: 0.21875, 0.99375\n",
            "Iteration 43900: 0.175, 0.9875\n",
            "Iteration 44000: 0.19375, 0.99375\n",
            "Validation results: 0.19375, 0.9625\n",
            "Iteration 44100: 0.1875, 0.99375\n",
            "Iteration 44200: 0.23125, 0.99375\n",
            "Iteration 44300: 0.18125, 1.0\n",
            "Iteration 44400: 0.14375, 1.0\n",
            "Iteration 44500: 0.21875, 1.0\n",
            "Validation results: 0.20625001, 1.0\n",
            "Iteration 44600: 0.2, 0.99375\n",
            "Iteration 44700: 0.2125, 0.99375\n",
            "Iteration 44800: 0.1875, 0.99375\n",
            "Iteration 44900: 0.23124999, 0.99375\n",
            "Iteration 45000: 0.16250001, 0.9875\n",
            "Validation results: 0.1625, 0.9875\n",
            "Iteration 45100: 0.1875, 0.99375\n",
            "Iteration 45200: 0.18125, 1.0\n",
            "Iteration 45300: 0.21875, 0.98125005\n",
            "Iteration 45400: 0.1875, 0.96875\n",
            "Iteration 45500: 0.19375, 1.0\n",
            "Validation results: 0.1375, 0.98125\n",
            "Iteration 45600: 0.2125, 1.0\n",
            "Iteration 45700: 0.24375, 0.9875\n",
            "Iteration 45800: 0.17500001, 0.99375\n",
            "Iteration 45900: 0.18125, 0.99375\n",
            "Iteration 46000: 0.15, 1.0\n",
            "Validation results: 0.23124999, 1.0\n",
            "Iteration 46100: 0.1875, 0.99375\n",
            "Iteration 46200: 0.15625, 0.98125\n",
            "Iteration 46300: 0.2, 1.0\n",
            "Iteration 46400: 0.2125, 0.9875\n",
            "Iteration 46500: 0.18750001, 0.9875\n",
            "Validation results: 0.1875, 0.975\n",
            "Iteration 46600: 0.17500001, 1.0\n",
            "Iteration 46700: 0.16875, 0.99375\n",
            "Iteration 46800: 0.19375, 1.0\n",
            "Iteration 46900: 0.1625, 1.0\n",
            "Iteration 47000: 0.19375, 0.9875\n",
            "Validation results: 0.2, 1.0\n",
            "Iteration 47100: 0.21875, 0.9875\n",
            "Iteration 47200: 0.18125, 0.99375\n",
            "Iteration 47300: 0.1375, 0.99375\n",
            "Iteration 47400: 0.1625, 0.975\n",
            "Iteration 47500: 0.1875, 0.975\n",
            "Validation results: 0.1875, 0.99375\n",
            "Iteration 47600: 0.24375, 0.9875\n",
            "Iteration 47700: 0.21875, 0.9875\n",
            "Iteration 47800: 0.175, 0.99375\n",
            "Iteration 47900: 0.17500001, 0.9875\n",
            "Iteration 48000: 0.19375, 0.99375\n",
            "Validation results: 0.1625, 1.0\n",
            "Iteration 48100: 0.1625, 0.9875\n",
            "Iteration 48200: 0.18125, 0.9875\n",
            "Iteration 48300: 0.1875, 0.98125005\n",
            "Iteration 48400: 0.1625, 1.0\n",
            "Iteration 48500: 0.225, 0.98125005\n",
            "Validation results: 0.17500001, 0.9625\n",
            "Iteration 48600: 0.20625001, 0.99375\n",
            "Iteration 48700: 0.16875, 0.98125\n",
            "Iteration 48800: 0.1625, 0.99375\n",
            "Iteration 48900: 0.23125002, 0.99375\n",
            "Iteration 49000: 0.1875, 0.9875\n",
            "Validation results: 0.2125, 0.98125005\n",
            "Iteration 49100: 0.19375, 0.9625\n",
            "Iteration 49200: 0.24375, 0.9875\n",
            "Iteration 49300: 0.19375, 0.98125005\n",
            "Iteration 49400: 0.17500001, 0.98125005\n",
            "Iteration 49500: 0.19375001, 1.0\n",
            "Validation results: 0.2125, 1.0\n",
            "Iteration 49600: 0.20625001, 0.9875\n",
            "Iteration 49700: 0.2, 0.99375\n",
            "Iteration 49800: 0.24375, 0.9875\n",
            "Iteration 49900: 0.2125, 0.99375\n",
            "Iteration 50000: 0.21875, 0.9875\n",
            "Validation results: 0.24375, 1.0\n",
            "Iteration 50100: 0.1375, 0.98125005\n",
            "Iteration 50200: 0.175, 0.975\n",
            "Iteration 50300: 0.2, 0.98125005\n",
            "Iteration 50400: 0.1875, 0.98125\n",
            "Iteration 50500: 0.17500001, 1.0\n",
            "Validation results: 0.16875, 0.9875\n",
            "Iteration 50600: 0.2, 0.99375\n",
            "Iteration 50700: 0.18125, 0.99375\n",
            "Iteration 50800: 0.1375, 0.975\n",
            "Iteration 50900: 0.2125, 1.0\n",
            "Iteration 51000: 0.25625, 0.99375\n",
            "Validation results: 0.19375, 0.9875\n",
            "Iteration 51100: 0.15, 0.98125\n",
            "Iteration 51200: 0.15625, 0.99375\n",
            "Iteration 51300: 0.1875, 0.99375\n",
            "Iteration 51400: 0.2375, 0.99375\n",
            "Iteration 51500: 0.2125, 0.99375\n",
            "Validation results: 0.2, 0.99375\n",
            "Iteration 51600: 0.19375001, 0.98125\n",
            "Iteration 51700: 0.175, 1.0\n",
            "Iteration 51800: 0.2125, 0.98125005\n",
            "Iteration 51900: 0.1875, 0.96875\n",
            "Iteration 52000: 0.225, 0.99375\n",
            "Validation results: 0.17500001, 0.99375\n",
            "Iteration 52100: 0.175, 0.9875\n",
            "Iteration 52200: 0.20625001, 0.9875\n",
            "Iteration 52300: 0.14375, 0.9875\n",
            "Iteration 52400: 0.19375001, 1.0\n",
            "Iteration 52500: 0.14375, 0.9875\n",
            "Validation results: 0.2, 1.0\n",
            "Iteration 52600: 0.18125, 1.0\n",
            "Iteration 52700: 0.17500001, 0.98125005\n",
            "Iteration 52800: 0.1875, 0.9875\n",
            "Iteration 52900: 0.18125, 0.99375\n",
            "Iteration 53000: 0.1875, 0.9875\n",
            "Validation results: 0.2375, 1.0\n",
            "Iteration 53100: 0.18125, 0.9875\n",
            "Iteration 53200: 0.1625, 0.98125005\n",
            "Iteration 53300: 0.20625001, 0.99375\n",
            "Iteration 53400: 0.18125, 0.9875\n",
            "Iteration 53500: 0.19375, 0.9875\n",
            "Validation results: 0.25, 0.99375\n",
            "Iteration 53600: 0.225, 0.9875\n",
            "Iteration 53700: 0.18125, 0.98125005\n",
            "Iteration 53800: 0.1875, 0.99375\n",
            "Iteration 53900: 0.2, 0.98125005\n",
            "Iteration 54000: 0.225, 1.0\n",
            "Validation results: 0.21875, 0.9875\n",
            "Iteration 54100: 0.16875, 1.0\n",
            "Iteration 54200: 0.21875, 0.99375\n",
            "Iteration 54300: 0.23125, 0.99375\n",
            "Iteration 54400: 0.2125, 0.98125\n",
            "Iteration 54500: 0.17500001, 0.98125005\n",
            "Validation results: 0.28750002, 0.96875\n",
            "Iteration 54600: 0.20625, 0.99375\n",
            "Iteration 54700: 0.1875, 0.98125005\n",
            "Iteration 54800: 0.19375001, 0.975\n",
            "Iteration 54900: 0.19375001, 0.9875\n",
            "Iteration 55000: 0.1625, 1.0\n",
            "Validation results: 0.15625, 0.975\n",
            "Iteration 55100: 0.19375, 0.98125\n",
            "Generating filenames\n",
            "Generating image processing ops\n",
            "Batching images\n",
            "Manipulating image data to be right shape\n",
            "Generating filenames\n",
            "Generating image processing ops\n",
            "Batching images\n",
            "Manipulating image data to be right shape\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0702 08:54:07.145343 140344355174272 deprecation.py:323] From <ipython-input-10-4b787753a554>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W0702 08:54:15.883780 140344355174272 deprecation.py:323] From <ipython-input-19-77db55fb89df>:108: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0702 08:54:32.607943 140344355174272 deprecation.py:323] From <ipython-input-17-1bfb9dc15133>:102: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done initializing, starting training.\n",
            "Iteration 100: 0.20625, 0.29375\n",
            "Iteration 200: 0.21875, 0.5\n",
            "Iteration 300: 0.2125, 0.59375\n",
            "Iteration 400: 0.16875, 0.55\n",
            "Iteration 500: 0.24374999, 0.54375005\n",
            "Validation results: 0.21875, 0.75625\n",
            "Iteration 600: 0.25, 0.75\n",
            "Iteration 700: 0.20625001, 0.66875005\n",
            "Iteration 800: 0.23125, 0.63125\n",
            "Iteration 900: 0.24375, 0.76250005\n",
            "Iteration 1000: 0.2, 0.7375\n",
            "Validation results: 0.16250001, 0.78125\n",
            "Iteration 1100: 0.21875, 0.70000005\n",
            "Iteration 1200: 0.21875, 0.78125\n",
            "Iteration 1300: 0.2375, 0.63125\n",
            "Iteration 1400: 0.25, 0.75625\n",
            "Iteration 1500: 0.14375001, 0.75\n",
            "Validation results: 0.14375001, 0.83125\n",
            "Iteration 1600: 0.20000002, 0.8375\n",
            "Iteration 1700: 0.29375, 0.775\n",
            "Iteration 1800: 0.19375, 0.84375\n",
            "Iteration 1900: 0.21875, 0.84375\n",
            "Iteration 2000: 0.19375, 0.83125\n",
            "Validation results: 0.17500001, 0.86875\n",
            "Iteration 2100: 0.20625001, 0.81875\n",
            "Iteration 2200: 0.15, 0.8375\n",
            "Iteration 2300: 0.25, 0.90625\n",
            "Iteration 2400: 0.2, 0.83125\n",
            "Iteration 2500: 0.22500001, 0.86875\n",
            "Validation results: 0.15625, 0.85625\n",
            "Iteration 2600: 0.1875, 0.8625\n",
            "Iteration 2700: 0.19375, 0.89374995\n",
            "Iteration 2800: 0.2125, 0.90625\n",
            "Iteration 2900: 0.17500001, 0.81875\n",
            "Iteration 3000: 0.23124999, 0.86249995\n",
            "Validation results: 0.125, 0.9125\n",
            "Iteration 3100: 0.19375, 0.84375\n",
            "Iteration 3200: 0.20625001, 0.90625\n",
            "Iteration 3300: 0.15, 0.85\n",
            "Iteration 3400: 0.20625001, 0.8875\n",
            "Iteration 3500: 0.18125, 0.88125\n",
            "Validation results: 0.20625001, 0.93125\n",
            "Iteration 3600: 0.1875, 0.8875\n",
            "Iteration 3700: 0.225, 0.925\n",
            "Iteration 3800: 0.20625001, 0.91875\n",
            "Iteration 3900: 0.1875, 0.89375\n",
            "Iteration 4000: 0.1875, 0.88124996\n",
            "Validation results: 0.2125, 0.925\n",
            "Iteration 4100: 0.23125, 0.93125\n",
            "Iteration 4200: 0.16875, 0.91249996\n",
            "Iteration 4300: 0.29375, 0.9\n",
            "Iteration 4400: 0.19375001, 0.90625\n",
            "Iteration 4500: 0.20625001, 0.91875\n",
            "Validation results: 0.1875, 0.93125\n",
            "Iteration 4600: 0.21875, 0.8375\n",
            "Iteration 4700: 0.1375, 0.91875\n",
            "Iteration 4800: 0.1625, 0.925\n",
            "Iteration 4900: 0.20625001, 0.93125\n",
            "Iteration 5000: 0.175, 0.9375\n",
            "Validation results: 0.1875, 0.90625\n",
            "Iteration 5100: 0.21875, 0.94375\n",
            "Iteration 5200: 0.21875, 0.9\n",
            "Iteration 5300: 0.23125, 0.9375\n",
            "Iteration 5400: 0.13749999, 0.89375\n",
            "Iteration 5500: 0.19375001, 0.9375\n",
            "Validation results: 0.1875, 0.95\n",
            "Iteration 5600: 0.2125, 0.91875\n",
            "Iteration 5700: 0.1625, 0.93125\n",
            "Iteration 5800: 0.24375, 0.95625\n",
            "Iteration 5900: 0.21875, 0.92499995\n",
            "Iteration 6000: 0.18125, 0.93125\n",
            "Validation results: 0.23125002, 0.92499995\n",
            "Iteration 6100: 0.2125, 0.975\n",
            "Iteration 6200: 0.2125, 0.92499995\n",
            "Iteration 6300: 0.23750001, 0.94375\n",
            "Iteration 6400: 0.19375001, 0.93125\n",
            "Iteration 6500: 0.1875, 0.95625\n",
            "Validation results: 0.19375, 0.95000005\n",
            "Iteration 6600: 0.1875, 0.98125005\n",
            "Iteration 6700: 0.23125, 0.93125\n",
            "Iteration 6800: 0.20625001, 0.93125\n",
            "Iteration 6900: 0.23125, 0.9625\n",
            "Iteration 7000: 0.20625, 0.9375\n",
            "Validation results: 0.19375, 0.93125\n",
            "Iteration 7100: 0.21875, 0.95\n",
            "Iteration 7200: 0.22500001, 0.94375\n",
            "Iteration 7300: 0.20000002, 0.95\n",
            "Iteration 7400: 0.2, 0.96875\n",
            "Iteration 7500: 0.18125, 0.9375\n",
            "Validation results: 0.18125, 0.96875\n",
            "Iteration 7600: 0.2375, 0.9625\n",
            "Iteration 7700: 0.225, 0.95624995\n",
            "Iteration 7800: 0.19375001, 0.95\n",
            "Iteration 7900: 0.2375, 0.9625\n",
            "Iteration 8000: 0.19375, 0.9625\n",
            "Validation results: 0.19375, 0.95625\n",
            "Iteration 8100: 0.16875, 0.95624995\n",
            "Iteration 8200: 0.15625, 0.96875\n",
            "Iteration 8300: 0.2, 0.94375\n",
            "Iteration 8400: 0.1875, 0.94375\n",
            "Iteration 8500: 0.2125, 0.92499995\n",
            "Validation results: 0.2125, 0.96875\n",
            "Iteration 8600: 0.25625, 0.96875\n",
            "Iteration 8700: 0.20625, 0.96875\n",
            "Iteration 8800: 0.17500001, 0.9375\n",
            "Iteration 8900: 0.20625001, 0.9625\n",
            "Iteration 9000: 0.20625001, 0.95625\n",
            "Validation results: 0.2, 0.95625\n",
            "Iteration 9100: 0.15, 0.98125005\n",
            "Iteration 9200: 0.2125, 0.9625\n",
            "Iteration 9300: 0.1875, 0.96875\n",
            "Iteration 9400: 0.20625, 0.91875005\n",
            "Iteration 9500: 0.17500001, 0.95000005\n",
            "Validation results: 0.21875001, 0.91875\n",
            "Iteration 9600: 0.17500001, 0.975\n",
            "Iteration 9700: 0.19375, 0.95624995\n",
            "Iteration 9800: 0.2, 0.975\n",
            "Iteration 9900: 0.20625, 0.95\n",
            "Iteration 10000: 0.19375, 0.95624995\n",
            "Validation results: 0.18125, 0.9625\n",
            "Iteration 10100: 0.18125, 0.925\n",
            "Iteration 10200: 0.13125, 0.975\n",
            "Iteration 10300: 0.2125, 0.95625\n",
            "Iteration 10400: 0.15, 0.975\n",
            "Iteration 10500: 0.19999999, 0.94375\n",
            "Validation results: 0.26875, 0.975\n",
            "Iteration 10600: 0.20625, 0.9625\n",
            "Iteration 10700: 0.19375001, 0.98125\n",
            "Iteration 10800: 0.175, 0.98125005\n",
            "Iteration 10900: 0.16250001, 0.96875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0702 10:20:56.908564 140344355174272 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 11000: 0.2125, 0.9625\n",
            "Validation results: 0.23750001, 0.9375\n",
            "Iteration 11100: 0.21875, 0.96250004\n",
            "Iteration 11200: 0.21875, 0.975\n",
            "Iteration 11300: 0.21875, 0.95625\n",
            "Iteration 11400: 0.17500001, 0.98125005\n",
            "Iteration 11500: 0.225, 0.9875\n",
            "Validation results: 0.18125, 0.9875\n",
            "Iteration 11600: 0.175, 0.91875005\n",
            "Iteration 11700: 0.19375, 0.96875\n",
            "Iteration 11800: 0.21875, 0.9625\n",
            "Iteration 11900: 0.16875002, 0.95624995\n",
            "Iteration 12000: 0.175, 0.96875\n",
            "Validation results: 0.225, 0.96875\n",
            "Iteration 12100: 0.22500001, 0.9875\n",
            "Iteration 12200: 0.2375, 0.9625\n",
            "Iteration 12300: 0.16875, 0.975\n",
            "Iteration 12400: 0.23750001, 0.98125\n",
            "Iteration 12500: 0.21875, 0.96875\n",
            "Validation results: 0.112500004, 0.96875\n",
            "Iteration 12600: 0.1625, 0.96875\n",
            "Iteration 12700: 0.14375001, 0.95\n",
            "Iteration 12800: 0.15625, 0.975\n",
            "Iteration 12900: 0.25625, 0.975\n",
            "Iteration 13000: 0.13749999, 0.98125005\n",
            "Validation results: 0.2125, 0.9875\n",
            "Iteration 13100: 0.19999999, 0.9875\n",
            "Iteration 13200: 0.21875, 0.98125005\n",
            "Iteration 13300: 0.1875, 0.975\n",
            "Iteration 13400: 0.21875, 0.99375\n",
            "Iteration 13500: 0.2125, 0.98125005\n",
            "Validation results: 0.16875, 0.98125005\n",
            "Iteration 13600: 0.20625001, 0.9875\n",
            "Iteration 13700: 0.175, 0.9875\n",
            "Iteration 13800: 0.15625, 0.975\n",
            "Iteration 13900: 0.17500001, 0.975\n",
            "Iteration 14000: 0.22500001, 0.99375\n",
            "Validation results: 0.15, 0.9875\n",
            "Iteration 14100: 0.14375001, 0.975\n",
            "Iteration 14200: 0.18125, 0.94374996\n",
            "Iteration 14300: 0.175, 0.94375\n",
            "Iteration 14400: 0.15, 0.94375\n",
            "Iteration 14500: 0.2, 0.96875\n",
            "Validation results: 0.19375001, 0.98125005\n",
            "Iteration 14600: 0.20000002, 0.98125\n",
            "Iteration 14700: 0.22500001, 0.9875\n",
            "Iteration 14800: 0.16875, 0.975\n",
            "Iteration 14900: 0.19375, 0.9375\n",
            "Iteration 15000: 0.16875, 0.975\n",
            "Validation results: 0.18125, 0.96250004\n",
            "Iteration 15100: 0.28125, 0.99375\n",
            "Iteration 15200: 0.2, 0.975\n",
            "Iteration 15300: 0.2, 0.98125005\n",
            "Iteration 15400: 0.225, 0.9875\n",
            "Iteration 15500: 0.1875, 0.98125005\n",
            "Validation results: 0.225, 1.0\n",
            "Iteration 15600: 0.16250001, 1.0\n",
            "Iteration 15700: 0.23125002, 0.94375\n",
            "Iteration 15800: 0.1875, 0.98125\n",
            "Iteration 15900: 0.20625, 0.9875\n",
            "Iteration 16000: 0.19375, 0.98125005\n",
            "Validation results: 0.16875, 0.96875\n",
            "Iteration 16100: 0.20625001, 0.95624995\n",
            "Iteration 16200: 0.2125, 0.975\n",
            "Iteration 16300: 0.10625, 0.95624995\n",
            "Iteration 16400: 0.2125, 0.95625\n",
            "Iteration 16500: 0.1875, 0.9625\n",
            "Validation results: 0.25, 0.96875\n",
            "Iteration 16600: 0.25, 0.9875\n",
            "Iteration 16700: 0.18125, 0.96875\n",
            "Iteration 16800: 0.19999999, 0.975\n",
            "Iteration 16900: 0.2, 0.99375\n",
            "Iteration 17000: 0.2, 0.975\n",
            "Validation results: 0.2125, 0.9875\n",
            "Iteration 17100: 0.21875, 0.98125005\n",
            "Iteration 17200: 0.20625, 0.95625\n",
            "Iteration 17300: 0.21875, 0.9875\n",
            "Iteration 17400: 0.1875, 0.9875\n",
            "Iteration 17500: 0.17500001, 0.975\n",
            "Validation results: 0.20000002, 0.9625\n",
            "Iteration 17600: 0.25, 0.96875\n",
            "Iteration 17700: 0.20625, 0.98125005\n",
            "Iteration 17800: 0.15, 0.9875\n",
            "Iteration 17900: 0.2125, 0.98125\n",
            "Iteration 18000: 0.22500001, 0.975\n",
            "Validation results: 0.23125, 1.0\n",
            "Iteration 18100: 0.19375, 0.975\n",
            "Iteration 18200: 0.17500001, 0.9625\n",
            "Iteration 18300: 0.22500001, 0.9875\n",
            "Iteration 18400: 0.21875001, 0.99375\n",
            "Iteration 18500: 0.21875, 0.9875\n",
            "Validation results: 0.16875, 0.94375\n",
            "Iteration 18600: 0.2, 0.99375\n",
            "Iteration 18700: 0.18125, 0.98125\n",
            "Iteration 18800: 0.24375, 0.9625\n",
            "Iteration 18900: 0.1875, 0.99375\n",
            "Iteration 19000: 0.18125, 0.9875\n",
            "Validation results: 0.17500001, 0.99375\n",
            "Iteration 19100: 0.13125001, 0.98125005\n",
            "Iteration 19200: 0.2, 0.975\n",
            "Iteration 19300: 0.20625, 0.975\n",
            "Iteration 19400: 0.18125, 0.9875\n",
            "Iteration 19500: 0.19375, 1.0\n",
            "Validation results: 0.24374999, 0.98125005\n",
            "Iteration 19600: 0.21875, 0.9875\n",
            "Iteration 19700: 0.19375, 0.96875\n",
            "Iteration 19800: 0.21249999, 0.98125005\n",
            "Iteration 19900: 0.20625, 0.9875\n",
            "Iteration 20000: 0.20625001, 0.975\n",
            "Validation results: 0.23125, 0.99375\n",
            "Iteration 20100: 0.25, 0.98125005\n",
            "Iteration 20200: 0.24375, 0.99375\n",
            "Iteration 20300: 0.19375, 0.95624995\n",
            "Iteration 20400: 0.19375001, 0.98125005\n",
            "Iteration 20500: 0.2125, 0.9875\n",
            "Validation results: 0.1875, 0.99375\n",
            "Iteration 20600: 0.21875, 0.96875\n",
            "Iteration 20700: 0.16875, 0.975\n",
            "Iteration 20800: 0.17500001, 0.9875\n",
            "Iteration 20900: 0.21875, 0.975\n",
            "Iteration 21000: 0.21875, 0.9875\n",
            "Validation results: 0.2, 0.9875\n",
            "Iteration 21100: 0.24375, 0.9875\n",
            "Iteration 21200: 0.23125, 1.0\n",
            "Iteration 21300: 0.16875, 0.99375\n",
            "Iteration 21400: 0.17500001, 0.99375\n",
            "Iteration 21500: 0.20625001, 0.96875\n",
            "Validation results: 0.16875, 0.98125005\n",
            "Iteration 21600: 0.18125, 0.9875\n",
            "Iteration 21700: 0.23125, 0.9875\n",
            "Iteration 21800: 0.20625001, 0.98125005\n",
            "Iteration 21900: 0.20625001, 0.98125005\n",
            "Iteration 22000: 0.15, 0.99375\n",
            "Validation results: 0.26875, 0.975\n",
            "Iteration 22100: 0.1625, 0.9875\n",
            "Iteration 22200: 0.22500001, 0.99375\n",
            "Iteration 22300: 0.19999999, 0.9875\n",
            "Iteration 22400: 0.20625, 0.99375\n",
            "Iteration 22500: 0.24375, 0.9875\n",
            "Validation results: 0.24375, 0.975\n",
            "Iteration 22600: 0.18125, 1.0\n",
            "Iteration 22700: 0.16874999, 0.975\n",
            "Iteration 22800: 0.19375, 0.9875\n",
            "Iteration 22900: 0.20625, 0.99375\n",
            "Iteration 23000: 0.19375, 0.975\n",
            "Validation results: 0.1625, 0.96875\n",
            "Iteration 23100: 0.15625, 1.0\n",
            "Iteration 23200: 0.19375001, 0.99375\n",
            "Iteration 23300: 0.20625, 0.9875\n",
            "Iteration 23400: 0.2, 0.98125\n",
            "Iteration 23500: 0.24375, 0.98125\n",
            "Validation results: 0.22500001, 1.0\n",
            "Iteration 23600: 0.16875, 0.98125005\n",
            "Iteration 23700: 0.1875, 0.975\n",
            "Iteration 23800: 0.2125, 0.99375\n",
            "Iteration 23900: 0.16250001, 0.9875\n",
            "Iteration 24000: 0.15625, 0.96875\n",
            "Validation results: 0.17500001, 0.96875\n",
            "Iteration 24100: 0.225, 0.9875\n",
            "Iteration 24200: 0.21875, 0.99375\n",
            "Iteration 24300: 0.19375, 0.9875\n",
            "Iteration 24400: 0.22500001, 0.99375\n",
            "Iteration 24500: 0.2375, 0.96875\n",
            "Validation results: 0.1875, 0.99375\n",
            "Iteration 24600: 0.2, 0.99375\n",
            "Iteration 24700: 0.15625, 0.95\n",
            "Iteration 24800: 0.225, 0.9875\n",
            "Iteration 24900: 0.23125, 1.0\n",
            "Iteration 25000: 0.21875, 0.99375\n",
            "Validation results: 0.17500001, 0.98125005\n",
            "Iteration 25100: 0.20625, 0.975\n",
            "Iteration 25200: 0.2125, 0.9875\n",
            "Iteration 25300: 0.18125, 0.99375\n",
            "Iteration 25400: 0.14375, 0.9375\n",
            "Iteration 25500: 0.2, 0.98125005\n",
            "Validation results: 0.1875, 0.99375\n",
            "Iteration 25600: 0.2125, 0.975\n",
            "Iteration 25700: 0.1875, 0.9875\n",
            "Iteration 25800: 0.15, 0.9875\n",
            "Iteration 25900: 0.16875002, 0.9875\n",
            "Iteration 26000: 0.16875, 0.975\n",
            "Validation results: 0.19375001, 0.975\n",
            "Iteration 26100: 0.1875, 0.9875\n",
            "Iteration 26200: 0.1875, 0.9875\n",
            "Iteration 26300: 0.19375, 0.9875\n",
            "Iteration 26400: 0.18125, 1.0\n",
            "Iteration 26500: 0.19999999, 1.0\n",
            "Validation results: 0.225, 0.98125005\n",
            "Iteration 26600: 0.23125, 0.99375\n",
            "Iteration 26700: 0.26875, 0.9875\n",
            "Iteration 26800: 0.2125, 0.99375\n",
            "Iteration 26900: 0.23125, 0.9875\n",
            "Iteration 27000: 0.20625, 0.98125005\n",
            "Validation results: 0.1375, 0.99375\n",
            "Iteration 27100: 0.1625, 0.98125005\n",
            "Iteration 27200: 0.19375, 0.99375\n",
            "Iteration 27300: 0.18125, 0.98125005\n",
            "Iteration 27400: 0.23125002, 0.98125\n",
            "Iteration 27500: 0.24375, 0.9625\n",
            "Validation results: 0.175, 0.975\n",
            "Iteration 27600: 0.2125, 0.9875\n",
            "Iteration 27700: 0.24374999, 0.98125005\n",
            "Iteration 27800: 0.19375001, 0.98125005\n",
            "Iteration 27900: 0.17500001, 0.9875\n",
            "Iteration 28000: 0.19375001, 0.975\n",
            "Validation results: 0.16875, 0.9875\n",
            "Iteration 28100: 0.21875, 0.9875\n",
            "Iteration 28200: 0.1875, 0.975\n",
            "Iteration 28300: 0.175, 0.9875\n",
            "Iteration 28400: 0.20000002, 1.0\n",
            "Iteration 28500: 0.1125, 0.98125005\n",
            "Validation results: 0.20625001, 0.99375\n",
            "Iteration 28600: 0.21875001, 0.98125005\n",
            "Iteration 28700: 0.18125, 0.99375\n",
            "Iteration 28800: 0.26250002, 0.9875\n",
            "Iteration 28900: 0.17500001, 0.99375\n",
            "Iteration 29000: 0.18125, 1.0\n",
            "Validation results: 0.175, 0.96875\n",
            "Iteration 29100: 0.175, 1.0\n",
            "Iteration 29200: 0.2125, 0.96875\n",
            "Iteration 29300: 0.2, 0.9875\n",
            "Iteration 29400: 0.2375, 0.99375\n",
            "Iteration 29500: 0.18125, 0.975\n",
            "Validation results: 0.1625, 0.99375\n",
            "Iteration 29600: 0.1875, 0.98125005\n",
            "Iteration 29700: 0.2375, 1.0\n",
            "Iteration 29800: 0.175, 0.99375\n",
            "Iteration 29900: 0.24375, 0.9875\n",
            "Iteration 30000: 0.1875, 0.975\n",
            "Validation results: 0.18125, 0.975\n",
            "Iteration 30100: 0.2125, 0.98125005\n",
            "Iteration 30200: 0.19375001, 0.99375\n",
            "Iteration 30300: 0.1875, 0.9875\n",
            "Iteration 30400: 0.15625, 0.9875\n",
            "Iteration 30500: 0.16250001, 0.9875\n",
            "Validation results: 0.2375, 0.99375\n",
            "Iteration 30600: 0.225, 1.0\n",
            "Iteration 30700: 0.20625001, 0.9875\n",
            "Iteration 30800: 0.1875, 0.99375\n",
            "Iteration 30900: 0.20000002, 0.9875\n",
            "Iteration 31000: 0.20625001, 0.9875\n",
            "Validation results: 0.1875, 0.98125\n",
            "Iteration 31100: 0.25625, 0.98125005\n",
            "Iteration 31200: 0.15, 0.99375\n",
            "Iteration 31300: 0.24374999, 0.9875\n",
            "Iteration 31400: 0.22500001, 0.99375\n",
            "Iteration 31500: 0.1625, 0.98125005\n",
            "Validation results: 0.118750006, 0.9875\n",
            "Iteration 31600: 0.23125, 0.975\n",
            "Iteration 31700: 0.21875, 0.9875\n",
            "Iteration 31800: 0.20625001, 0.99375\n",
            "Iteration 31900: 0.15625, 0.99375\n",
            "Iteration 32000: 0.23750001, 0.9875\n",
            "Validation results: 0.16250001, 0.99375\n",
            "Iteration 32100: 0.15, 0.975\n",
            "Iteration 32200: 0.18125, 0.98125005\n",
            "Iteration 32300: 0.1875, 0.99375\n",
            "Iteration 32400: 0.16875, 0.99375\n",
            "Iteration 32500: 0.23125, 0.9875\n",
            "Validation results: 0.20625001, 0.98125005\n",
            "Iteration 32600: 0.2125, 0.9875\n",
            "Iteration 32700: 0.2125, 0.98125\n",
            "Iteration 32800: 0.2, 0.9625\n",
            "Iteration 32900: 0.1875, 0.99375\n",
            "Iteration 33000: 0.15, 0.99375\n",
            "Validation results: 0.2, 0.9875\n",
            "Iteration 33100: 0.1625, 0.99375\n",
            "Iteration 33200: 0.125, 0.99375\n",
            "Iteration 33300: 0.25625002, 0.9875\n",
            "Iteration 33400: 0.2125, 0.9875\n",
            "Iteration 33500: 0.20625001, 0.9875\n",
            "Validation results: 0.19375001, 1.0\n",
            "Iteration 33600: 0.15625, 0.99375\n",
            "Iteration 33700: 0.20625001, 0.99375\n",
            "Iteration 33800: 0.175, 0.98125005\n",
            "Iteration 33900: 0.20000002, 0.98125005\n",
            "Iteration 34000: 0.1875, 1.0\n",
            "Validation results: 0.15625, 0.975\n",
            "Iteration 34100: 0.23750001, 0.98125005\n",
            "Iteration 34200: 0.225, 0.99375\n",
            "Iteration 34300: 0.21875, 0.99375\n",
            "Iteration 34400: 0.16875002, 0.99375\n",
            "Iteration 34500: 0.20625001, 1.0\n",
            "Validation results: 0.21875, 0.98125005\n",
            "Iteration 34600: 0.23750001, 0.98125005\n",
            "Iteration 34700: 0.19375, 1.0\n",
            "Iteration 34800: 0.19375001, 0.9875\n",
            "Iteration 34900: 0.15625, 0.99375\n",
            "Iteration 35000: 0.18125, 0.99375\n",
            "Validation results: 0.225, 0.99375\n",
            "Iteration 35100: 0.20625001, 1.0\n",
            "Iteration 35200: 0.15625, 0.98125005\n",
            "Iteration 35300: 0.18125, 0.975\n",
            "Iteration 35400: 0.2, 0.99375\n",
            "Iteration 35500: 0.2375, 0.9875\n",
            "Validation results: 0.16875, 0.98125005\n",
            "Iteration 35600: 0.19999999, 0.975\n",
            "Iteration 35700: 0.19375001, 0.99375\n",
            "Iteration 35800: 0.10625, 0.9875\n",
            "Iteration 35900: 0.1875, 0.99375\n",
            "Iteration 36000: 0.18125, 1.0\n",
            "Validation results: 0.19999999, 0.99375\n",
            "Iteration 36100: 0.21875, 0.9875\n",
            "Iteration 36200: 0.18125, 0.9875\n",
            "Iteration 36300: 0.1875, 0.98125005\n",
            "Iteration 36400: 0.18125, 0.98125005\n",
            "Iteration 36500: 0.2125, 0.99375\n",
            "Validation results: 0.1875, 0.98125\n",
            "Iteration 36600: 0.2375, 0.9875\n",
            "Iteration 36700: 0.1625, 0.99375\n",
            "Iteration 36800: 0.2625, 0.96875\n",
            "Iteration 36900: 0.16250001, 0.98125\n",
            "Iteration 37000: 0.15625, 0.99375\n",
            "Validation results: 0.15625, 0.99375\n",
            "Iteration 37100: 0.19375001, 0.98125\n",
            "Iteration 37200: 0.23750001, 0.99375\n",
            "Iteration 37300: 0.18750001, 0.99375\n",
            "Iteration 37400: 0.19999999, 0.9875\n",
            "Iteration 37500: 0.23124999, 0.98125005\n",
            "Validation results: 0.17500001, 0.99375\n",
            "Iteration 37600: 0.23750001, 0.99375\n",
            "Iteration 37700: 0.23124999, 0.9875\n",
            "Iteration 37800: 0.2125, 1.0\n",
            "Iteration 37900: 0.21875, 0.9875\n",
            "Iteration 38000: 0.1875, 0.99375\n",
            "Validation results: 0.175, 0.9875\n",
            "Iteration 38100: 0.2, 0.99375\n",
            "Iteration 38200: 0.15, 0.99375\n",
            "Iteration 38300: 0.23750001, 0.96875\n",
            "Iteration 38400: 0.225, 0.99375\n",
            "Iteration 38500: 0.19375, 0.9875\n",
            "Validation results: 0.2, 0.975\n",
            "Iteration 38600: 0.225, 0.98125005\n",
            "Iteration 38700: 0.18125, 1.0\n",
            "Iteration 38800: 0.16250001, 0.96875\n",
            "Iteration 38900: 0.21875, 0.9875\n",
            "Iteration 39000: 0.25, 0.9625\n",
            "Validation results: 0.19375, 0.98125\n",
            "Iteration 39100: 0.13125, 0.9875\n",
            "Iteration 39200: 0.2125, 0.98125005\n",
            "Iteration 39300: 0.2125, 1.0\n",
            "Iteration 39400: 0.2, 0.99375\n",
            "Iteration 39500: 0.19375001, 0.99375\n",
            "Validation results: 0.21875, 0.9875\n",
            "Iteration 39600: 0.18125, 0.99375\n",
            "Iteration 39700: 0.23125, 0.99375\n",
            "Iteration 39800: 0.16875, 0.98125\n",
            "Iteration 39900: 0.15625, 1.0\n",
            "Iteration 40000: 0.23750001, 0.96875\n",
            "Validation results: 0.20625001, 0.99375\n",
            "Iteration 40100: 0.18125, 0.99375\n",
            "Iteration 40200: 0.23124999, 0.99375\n",
            "Iteration 40300: 0.16875, 0.9875\n",
            "Iteration 40400: 0.26875, 0.98125005\n",
            "Iteration 40500: 0.1875, 0.99375\n",
            "Validation results: 0.16875, 1.0\n",
            "Iteration 40600: 0.15, 0.9875\n",
            "Iteration 40700: 0.24375, 1.0\n",
            "Iteration 40800: 0.23125002, 0.99375\n",
            "Iteration 40900: 0.2125, 0.9875\n",
            "Iteration 41000: 0.16875002, 0.9875\n",
            "Validation results: 0.17500001, 0.98125005\n",
            "Iteration 41100: 0.24375, 1.0\n",
            "Iteration 41200: 0.18125, 0.98125005\n",
            "Iteration 41300: 0.225, 0.9875\n",
            "Iteration 41400: 0.19375001, 0.99375\n",
            "Iteration 41500: 0.2125, 0.99375\n",
            "Validation results: 0.225, 0.99375\n",
            "Iteration 41600: 0.24375, 0.9875\n",
            "Iteration 41700: 0.23125, 0.98125\n",
            "Iteration 41800: 0.24375, 0.98125005\n",
            "Iteration 41900: 0.18125, 0.98125\n",
            "Iteration 42000: 0.14375001, 0.975\n",
            "Validation results: 0.1875, 0.99375\n",
            "Iteration 42100: 0.2, 0.98125005\n",
            "Iteration 42200: 0.1875, 0.98125005\n",
            "Iteration 42300: 0.23125002, 0.96875\n",
            "Iteration 42400: 0.20625001, 0.99375\n",
            "Iteration 42500: 0.23125002, 0.99375\n",
            "Validation results: 0.16875002, 0.98125005\n",
            "Iteration 42600: 0.18125, 0.99375\n",
            "Iteration 42700: 0.1875, 1.0\n",
            "Iteration 42800: 0.24375, 0.9625\n",
            "Iteration 42900: 0.1625, 0.98125\n",
            "Iteration 43000: 0.2125, 0.99375\n",
            "Validation results: 0.16250001, 0.99375\n",
            "Iteration 43100: 0.16250001, 0.99375\n",
            "Iteration 43200: 0.21875, 0.9875\n",
            "Iteration 43300: 0.24375, 0.99375\n",
            "Iteration 43400: 0.19375, 1.0\n",
            "Iteration 43500: 0.15625, 0.9875\n",
            "Validation results: 0.118750006, 0.99375\n",
            "Iteration 43600: 0.26875, 1.0\n",
            "Iteration 43700: 0.19375, 0.99375\n",
            "Iteration 43800: 0.21875, 0.99375\n",
            "Iteration 43900: 0.175, 0.9875\n",
            "Iteration 44000: 0.19375, 0.99375\n",
            "Validation results: 0.19375, 0.9625\n",
            "Iteration 44100: 0.1875, 0.99375\n",
            "Iteration 44200: 0.23125, 0.99375\n",
            "Iteration 44300: 0.18125, 1.0\n",
            "Iteration 44400: 0.14375, 1.0\n",
            "Iteration 44500: 0.21875, 1.0\n",
            "Validation results: 0.20625001, 1.0\n",
            "Iteration 44600: 0.2, 0.99375\n",
            "Iteration 44700: 0.2125, 0.99375\n",
            "Iteration 44800: 0.1875, 0.99375\n",
            "Iteration 44900: 0.23124999, 0.99375\n",
            "Iteration 45000: 0.16250001, 0.9875\n",
            "Validation results: 0.1625, 0.9875\n",
            "Iteration 45100: 0.1875, 0.99375\n",
            "Iteration 45200: 0.18125, 1.0\n",
            "Iteration 45300: 0.21875, 0.98125005\n",
            "Iteration 45400: 0.1875, 0.96875\n",
            "Iteration 45500: 0.19375, 1.0\n",
            "Validation results: 0.1375, 0.98125\n",
            "Iteration 45600: 0.2125, 1.0\n",
            "Iteration 45700: 0.24375, 0.9875\n",
            "Iteration 45800: 0.17500001, 0.99375\n",
            "Iteration 45900: 0.18125, 0.99375\n",
            "Iteration 46000: 0.15, 1.0\n",
            "Validation results: 0.23124999, 1.0\n",
            "Iteration 46100: 0.1875, 0.99375\n",
            "Iteration 46200: 0.15625, 0.98125\n",
            "Iteration 46300: 0.2, 1.0\n",
            "Iteration 46400: 0.2125, 0.9875\n",
            "Iteration 46500: 0.18750001, 0.9875\n",
            "Validation results: 0.1875, 0.975\n",
            "Iteration 46600: 0.17500001, 1.0\n",
            "Iteration 46700: 0.16875, 0.99375\n",
            "Iteration 46800: 0.19375, 1.0\n",
            "Iteration 46900: 0.1625, 1.0\n",
            "Iteration 47000: 0.19375, 0.9875\n",
            "Validation results: 0.2, 1.0\n",
            "Iteration 47100: 0.21875, 0.9875\n",
            "Iteration 47200: 0.18125, 0.99375\n",
            "Iteration 47300: 0.1375, 0.99375\n",
            "Iteration 47400: 0.1625, 0.975\n",
            "Iteration 47500: 0.1875, 0.975\n",
            "Validation results: 0.1875, 0.99375\n",
            "Iteration 47600: 0.24375, 0.9875\n",
            "Iteration 47700: 0.21875, 0.9875\n",
            "Iteration 47800: 0.175, 0.99375\n",
            "Iteration 47900: 0.17500001, 0.9875\n",
            "Iteration 48000: 0.19375, 0.99375\n",
            "Validation results: 0.1625, 1.0\n",
            "Iteration 48100: 0.1625, 0.9875\n",
            "Iteration 48200: 0.18125, 0.9875\n",
            "Iteration 48300: 0.1875, 0.98125005\n",
            "Iteration 48400: 0.1625, 1.0\n",
            "Iteration 48500: 0.225, 0.98125005\n",
            "Validation results: 0.17500001, 0.9625\n",
            "Iteration 48600: 0.20625001, 0.99375\n",
            "Iteration 48700: 0.16875, 0.98125\n",
            "Iteration 48800: 0.1625, 0.99375\n",
            "Iteration 48900: 0.23125002, 0.99375\n",
            "Iteration 49000: 0.1875, 0.9875\n",
            "Validation results: 0.2125, 0.98125005\n",
            "Iteration 49100: 0.19375, 0.9625\n",
            "Iteration 49200: 0.24375, 0.9875\n",
            "Iteration 49300: 0.19375, 0.98125005\n",
            "Iteration 49400: 0.17500001, 0.98125005\n",
            "Iteration 49500: 0.19375001, 1.0\n",
            "Validation results: 0.2125, 1.0\n",
            "Iteration 49600: 0.20625001, 0.9875\n",
            "Iteration 49700: 0.2, 0.99375\n",
            "Iteration 49800: 0.24375, 0.9875\n",
            "Iteration 49900: 0.2125, 0.99375\n",
            "Iteration 50000: 0.21875, 0.9875\n",
            "Validation results: 0.24375, 1.0\n",
            "Iteration 50100: 0.1375, 0.98125005\n",
            "Iteration 50200: 0.175, 0.975\n",
            "Iteration 50300: 0.2, 0.98125005\n",
            "Iteration 50400: 0.1875, 0.98125\n",
            "Iteration 50500: 0.17500001, 1.0\n",
            "Validation results: 0.16875, 0.9875\n",
            "Iteration 50600: 0.2, 0.99375\n",
            "Iteration 50700: 0.18125, 0.99375\n",
            "Iteration 50800: 0.1375, 0.975\n",
            "Iteration 50900: 0.2125, 1.0\n",
            "Iteration 51000: 0.25625, 0.99375\n",
            "Validation results: 0.19375, 0.9875\n",
            "Iteration 51100: 0.15, 0.98125\n",
            "Iteration 51200: 0.15625, 0.99375\n",
            "Iteration 51300: 0.1875, 0.99375\n",
            "Iteration 51400: 0.2375, 0.99375\n",
            "Iteration 51500: 0.2125, 0.99375\n",
            "Validation results: 0.2, 0.99375\n",
            "Iteration 51600: 0.19375001, 0.98125\n",
            "Iteration 51700: 0.175, 1.0\n",
            "Iteration 51800: 0.2125, 0.98125005\n",
            "Iteration 51900: 0.1875, 0.96875\n",
            "Iteration 52000: 0.225, 0.99375\n",
            "Validation results: 0.17500001, 0.99375\n",
            "Iteration 52100: 0.175, 0.9875\n",
            "Iteration 52200: 0.20625001, 0.9875\n",
            "Iteration 52300: 0.14375, 0.9875\n",
            "Iteration 52400: 0.19375001, 1.0\n",
            "Iteration 52500: 0.14375, 0.9875\n",
            "Validation results: 0.2, 1.0\n",
            "Iteration 52600: 0.18125, 1.0\n",
            "Iteration 52700: 0.17500001, 0.98125005\n",
            "Iteration 52800: 0.1875, 0.9875\n",
            "Iteration 52900: 0.18125, 0.99375\n",
            "Iteration 53000: 0.1875, 0.9875\n",
            "Validation results: 0.2375, 1.0\n",
            "Iteration 53100: 0.18125, 0.9875\n",
            "Iteration 53200: 0.1625, 0.98125005\n",
            "Iteration 53300: 0.20625001, 0.99375\n",
            "Iteration 53400: 0.18125, 0.9875\n",
            "Iteration 53500: 0.19375, 0.9875\n",
            "Validation results: 0.25, 0.99375\n",
            "Iteration 53600: 0.225, 0.9875\n",
            "Iteration 53700: 0.18125, 0.98125005\n",
            "Iteration 53800: 0.1875, 0.99375\n",
            "Iteration 53900: 0.2, 0.98125005\n",
            "Iteration 54000: 0.225, 1.0\n",
            "Validation results: 0.21875, 0.9875\n",
            "Iteration 54100: 0.16875, 1.0\n",
            "Iteration 54200: 0.21875, 0.99375\n",
            "Iteration 54300: 0.23125, 0.99375\n",
            "Iteration 54400: 0.2125, 0.98125\n",
            "Iteration 54500: 0.17500001, 0.98125005\n",
            "Validation results: 0.28750002, 0.96875\n",
            "Iteration 54600: 0.20625, 0.99375\n",
            "Iteration 54700: 0.1875, 0.98125005\n",
            "Iteration 54800: 0.19375001, 0.975\n",
            "Iteration 54900: 0.19375001, 0.9875\n",
            "Iteration 55000: 0.1625, 1.0\n",
            "Validation results: 0.15625, 0.975\n",
            "Iteration 55100: 0.19375, 0.98125\n",
            "Iteration 55200: 0.225, 0.99375\n",
            "Iteration 55200: 0.225, 0.99375\n",
            "Iteration 55300: 0.2375, 0.9875\n",
            "Iteration 55300: 0.2375, 0.9875\n",
            "Iteration 55400: 0.20000002, 0.98125005\n",
            "Iteration 55400: 0.20000002, 0.98125005\n",
            "Iteration 55500: 0.22500001, 0.98125\n",
            "Iteration 55500: 0.22500001, 0.98125\n",
            "Validation results: 0.20625001, 1.0\n",
            "Validation results: 0.20625001, 1.0\n",
            "Iteration 55600: 0.16874999, 1.0\n",
            "Iteration 55600: 0.16874999, 1.0\n",
            "Iteration 55700: 0.23125, 0.9875\n",
            "Iteration 55700: 0.23125, 0.9875\n",
            "Iteration 55800: 0.14375001, 0.99375\n",
            "Iteration 55800: 0.14375001, 0.99375\n",
            "Iteration 55900: 0.1625, 0.98125005\n",
            "Iteration 55900: 0.1625, 0.98125005\n",
            "Iteration 56000: 0.24375002, 0.98125005\n",
            "Iteration 56000: 0.24375002, 0.98125005\n",
            "Validation results: 0.2, 0.98125005\n",
            "Validation results: 0.2, 0.98125005\n",
            "Iteration 56100: 0.18125, 0.975\n",
            "Iteration 56100: 0.18125, 0.975\n",
            "Iteration 56200: 0.1875, 0.9875\n",
            "Iteration 56200: 0.1875, 0.9875\n",
            "Iteration 56300: 0.15625, 0.975\n",
            "Iteration 56300: 0.15625, 0.975\n",
            "Iteration 56400: 0.175, 1.0\n",
            "Iteration 56400: 0.175, 1.0\n",
            "Iteration 56500: 0.26875, 0.99375\n",
            "Iteration 56500: 0.26875, 0.99375\n",
            "Validation results: 0.22500001, 0.9875\n",
            "Validation results: 0.22500001, 0.9875\n",
            "Iteration 56600: 0.19375001, 0.96875\n",
            "Iteration 56600: 0.19375001, 0.96875\n",
            "Iteration 56700: 0.1625, 0.9875\n",
            "Iteration 56700: 0.1625, 0.9875\n",
            "Iteration 56800: 0.2, 0.96875\n",
            "Iteration 56800: 0.2, 0.96875\n",
            "Iteration 56900: 0.19375, 0.98125005\n",
            "Iteration 56900: 0.19375, 0.98125005\n",
            "Iteration 57000: 0.21875, 0.98125005\n",
            "Iteration 57000: 0.21875, 0.98125005\n",
            "Validation results: 0.23750001, 0.9875\n",
            "Validation results: 0.23750001, 0.9875\n",
            "Iteration 57100: 0.21875, 0.975\n",
            "Iteration 57100: 0.21875, 0.975\n",
            "Iteration 57200: 0.1375, 0.95\n",
            "Iteration 57200: 0.1375, 0.95\n",
            "Iteration 57300: 0.2, 0.975\n",
            "Iteration 57300: 0.2, 0.975\n",
            "Iteration 57400: 0.23124999, 0.98125005\n",
            "Iteration 57400: 0.23124999, 0.98125005\n",
            "Iteration 57500: 0.175, 0.9875\n",
            "Iteration 57500: 0.175, 0.9875\n",
            "Validation results: 0.19375, 0.9875\n",
            "Validation results: 0.19375, 0.9875\n",
            "Iteration 57600: 0.25, 0.98125\n",
            "Iteration 57600: 0.25, 0.98125\n",
            "Iteration 57700: 0.1875, 0.99375\n",
            "Iteration 57700: 0.1875, 0.99375\n",
            "Iteration 57800: 0.1875, 0.975\n",
            "Iteration 57800: 0.1875, 0.975\n",
            "Iteration 57900: 0.26875, 0.9875\n",
            "Iteration 57900: 0.26875, 0.9875\n",
            "Iteration 58000: 0.21875, 0.99375\n",
            "Iteration 58000: 0.21875, 0.99375\n",
            "Validation results: 0.21875, 0.9875\n",
            "Validation results: 0.21875, 0.9875\n",
            "Iteration 58100: 0.18125, 0.98125005\n",
            "Iteration 58100: 0.18125, 0.98125005\n",
            "Iteration 58200: 0.25, 0.9625\n",
            "Iteration 58200: 0.25, 0.9625\n",
            "Iteration 58300: 0.21875, 0.9625\n",
            "Iteration 58300: 0.21875, 0.9625\n",
            "Iteration 58400: 0.18125, 0.98125005\n",
            "Iteration 58400: 0.18125, 0.98125005\n",
            "Iteration 58500: 0.18125, 0.9875\n",
            "Iteration 58500: 0.18125, 0.9875\n",
            "Validation results: 0.20625, 0.99375\n",
            "Validation results: 0.20625, 0.99375\n",
            "Iteration 58600: 0.19375001, 0.96875\n",
            "Iteration 58600: 0.19375001, 0.96875\n",
            "Iteration 58700: 0.1625, 1.0\n",
            "Iteration 58700: 0.1625, 1.0\n",
            "Iteration 58800: 0.15625, 0.98125005\n",
            "Iteration 58800: 0.15625, 0.98125005\n",
            "Iteration 58900: 0.2, 0.9875\n",
            "Iteration 58900: 0.2, 0.9875\n",
            "Iteration 59000: 0.22500001, 0.98125005\n",
            "Iteration 59000: 0.22500001, 0.98125005\n",
            "Validation results: 0.17500001, 0.96875\n",
            "Validation results: 0.17500001, 0.96875\n",
            "Iteration 59100: 0.1875, 0.99375\n",
            "Iteration 59100: 0.1875, 0.99375\n",
            "Iteration 59200: 0.21249999, 0.98125005\n",
            "Iteration 59200: 0.21249999, 0.98125005\n",
            "Iteration 59300: 0.24375, 0.9625\n",
            "Iteration 59300: 0.24375, 0.9625\n",
            "Iteration 59400: 0.13125, 0.9875\n",
            "Iteration 59400: 0.13125, 0.9875\n",
            "Iteration 59500: 0.1875, 1.0\n",
            "Iteration 59500: 0.1875, 1.0\n",
            "Validation results: 0.21875, 0.96875\n",
            "Validation results: 0.21875, 0.96875\n",
            "Iteration 59600: 0.14375001, 0.99375\n",
            "Iteration 59600: 0.14375001, 0.99375\n",
            "Iteration 59700: 0.19375001, 1.0\n",
            "Iteration 59700: 0.19375001, 1.0\n",
            "Iteration 59800: 0.13125001, 0.9875\n",
            "Iteration 59800: 0.13125001, 0.9875\n",
            "Iteration 59900: 0.1875, 0.99375\n",
            "Iteration 59900: 0.1875, 0.99375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PQNYF34QX3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir logs\n",
        "!mkdir logs/omniglot5way/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQeO9OaS59hw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHuZng08aif3",
        "colab_type": "code",
        "outputId": "7da34125-6764-4e87-eec0-84185185e621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WulAgfOlXqa",
        "colab_type": "code",
        "outputId": "3f568fd8-8fba-4693-92a6-fbd594e4f4da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!du -h logs/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.1G\tlogs/omniglot5way/cls_5.mbs_32.ubs_1.numstep1.updatelr0.4batchnorm\n",
            "3.1G\tlogs/omniglot5way\n",
            "3.1G\tlogs/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shFgIVOnll-P",
        "colab_type": "code",
        "outputId": "108f25fe-0567-45f5-9299-55c777d21e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zB5HrTEmtjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r logs/ drive/My\\ Drive/MAML\\ logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlDlQH6mtvSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}